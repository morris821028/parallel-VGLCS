\section{Implementation Optimization} \label{sec:Implementation}

This section describes the optimization in our implementations of VGLCS
algorithms, both in sequential and parallel environments.  Note that
some of these techniques address hardware characteristics, e.g., cache
behavior, and are not confined to asymptotically analysis only.

\subsection{The Strategy of Disjoint Set Implementation}

We now describe the optimization in the implementations of disjoint
sets, whose applications include VGLCS.

%% There two techniques to efficiently merge two disjoint sets -- {\em
%%   path compression}, and {\em rank strategy}.

\subsubsection{Cache Performance}

The cache is essential to efficient implementation of disjoint sets.
Patwary, Blair, and Manne~\cite{Patwary2010ExperimentsOU} conducted
experiments on disjoint-set and showed that different implementations
have different impact on different levels of caches misses.  In
practice the cache miss is strongly related to how we go from a child
to its ancestors through pointer chasing during path compression.
Usually, an algorithm with a lower time complexity, e.g.,  will have more
``long jumps'' than an algorithm with a higher time complexity.  Here
the long jump means the pointer chasing will go from a memory address
to a far away memory address.  Please refer to
Figure~\ref{fig:long-short-jump-disjoint} for an illustration.

\begin{figure}[!thb]
  \centering \subfigure[An algorithm with a lower time complexity] {
    \includegraphics[width=0.42\linewidth]{\GraphicPath/fig-rem-long-jump.pdf}
  } \subfigure[An algorithm with a higher time complexity] {
    \includegraphics[width=0.42\linewidth]{\GraphicPath/fig-rem-short-jump.pdf}
  }
  \caption{The parent jump in disjoint set}
  \label{fig:long-short-jump-disjoint}
\end{figure}

% what is the full name for RemSP? What does it mean???

The Rem's algorithm ({\sc Rem})~\cite{dijkstra1976a} achieves better
cache performance by a {\em merge-by-index} technique.  Traditional
disjoint set merging techniques are {\em merge-by-rank}\cite{XXX} and
{\em merge-by-size}\cite{XXX}, which determine the root of new tree by
the ranks and the sizes of the two trees respectively.  Despite that
they do have asymptotically better time complexity, put their better
time complexity here.  However, their performance in practice is not
so impressive due to the previously mentioned cache issue.  In
contrast Rem's algorithm assigns an {\em index} to each node, and
merge two disjoint sets according the index of the roots of the two
trees being merged.  That is, the root of the new merged tree will be
the root with the ``larger'' index.

In our experiments we still use merge-by-rank to merge disjoint set
trees.  However, when we have two {\em equally ranked} disjoint set
trees to merge, we will apply the {\em merge-by-index} technique to
break the tie, so as to improve cache performance.  Experiments show
that the merge-by-index technique improves by up to 5 \%.

\subsubsection{Application on VGLCS}

% VGLCS

It is possible to further optimize the implementation of Peng's
sequential algorithm (Algorithm~\ref{alg:serial-VGLCS}) by a {\em lazy
  insertion} technique.  Recall that in
Algorithm~\ref{alg:serial-VGLCS} when the $i$-th characters from one
input string does not match the $j$-th character of the other input
string, we need to place a zero into the dynamic programming table at
position $V[i][j]$.  In practice this this mismatch will happen very
frequently, and causing frequent insertions of zeros into the disjoint
sets.  Each insertion then links the newly inserted singleton node of
zero the previous node of zero that was inserted one iteration ago. 

We implemented an optimization that resolves this repeated insertion,
and linking, of zeros.  Our implementation will scan through a series
of zeros, and locate the next non-zero $V$ element (denoted as $v$) in
the same column, and insert them as a batch.  Since all values in $V$
are non-negative, and hence larger than zero, we can link all zeros to
$v$.  From the experiments we observe that this technique causes less
pointer chasing and updates and does improve performance.

Note that we do {\em not} apply the lazy insertion optimization in our
parallel implementation.  For a multi-core platform, the efficiency of
thread synchronization is essential to the performance.  Since the
threads of a parallel VGLCS algorithm needs to synchronize at every
column of the dynamic programming table in order to process a row, the
lazy insertion, which can ignore this dependency in a sequential
environment, is not beneficial, and is not adopted in our parallel
implementation.

\subsection{Parallel Range Query in VGLCS}

%% %???

%% In the VGLCS problem, the information of range query can be reused a
%% lots of times.  We can reduce the amount of computation for our
%% dynamic programming problem by remove duplicate computation and
%% imposing a boundary limitation.  For example, the logarithm function
%% is often used in the query of a sparse table, and we can preprocessing
%% all the result of the requirements.  Therefore, the reduce-boundary
%% Algorithm~\ref{alg:reduce-boundary} runs $O(n \log n)$ time, which $n$
%% is the length of the input sequence.  It would not increase the time
%% complexity because the VGLCS problem is solved in $O(n^2 / p + n \log
%% n)$, which $p$ is the number of the processors.

%% \iffalse
%% 在 VGLCS 問題中，平行區間詢問的每個區間範圍都是已知的，每一個會重複使用好幾次，
%% 由於已知詢問的區間資訊，我們可以藉由建表範圍縮小，
%% 從算法 \ref{alg:reduce-boundary} 推導邊界來減少計算量。
%% \fi

%% \input{\AlgoPath/alg-reduce-boundary-2e}

We further improve the performance of range query by maintaining
several tables in the blocked sparse table approach described in
Section~\ref{sec:blocked-sparse-table}.  There are three tables --
block maximum $T_S$, prefix maximum $P$, and suffix maximum $S$.  Each
entry in the block maximum table is the maximum within that block.  As
described in Section~\ref{sec:blocked-sparse-table}, we also maintain
a sparse table on $T_S$ so that we can easily answer ranged query on
$B$, which is equal to the maximum within several {\em consecutive}
blocks from the input data.  The prefix maximum table $P$ maintains
the maximum of the prefix within a block, and the suffix maximum table
$S$ maintains the maximum for prefix.  It is easy to see that any
range query can be answered by two queries into the sparse table on
$B$, and one query into prefix $P$, and one into $S$.  For example, in
Figure~\ref{fig:compressed-sp-opt}, the query from index 2 to 18 can
be broken into two queries into the sparse table for $T_S$ -- one from
block 1 to block 2, and one from 2 to 3, and one query for the suffix
of length 2 into the first block , and one query for the prefix of
length 3 into the last block.  Please refer to
Figure~\ref{fig:compressed-sp-opt} for an illustration.

%% The in-block query is a very small probability event due to small $s =
%% \frac{\log n}{4}$.

%% We can use prefix and suffix maximum array to reduce the number of
%% lookup operation on LCA table.

\begin{figure}[!thb]
  \centering \subfigure[The prefix/suffix maximum tables for blocks] {
    \includegraphics[width=0.50\linewidth]{\GraphicPath/fig-compressed-sp-prefix-suffix.pdf}
  } \subfigure[A Sparse Table for $T_S$] {
    \includegraphics[width=0.4\linewidth]{\GraphicPath/fig-sparse-table.pdf}
  } \caption{Block maximum $T_S$, prefix maximum $P$, and suffix
    maximum $S$.}
  \label{fig:compressed-sp-opt}
\end{figure}

We argue that the {\em order} to access these maximum tables is
important.  Our implementation accesses the block maximum {\em first},
then the prefix maximum {\em second}, then the suffix maximum {\em
  last}.  The reasoning for this order is as follows.  Since we need
to access two elements in the sparse table for $T_S$ in {\em the same
  level}, it is very likely they will be in the same cache line, so
access the first will bring in the other automatically by hardware
cache.  For example, in Figure~\ref{fig:compressed-sp-opt}, we will
access both $T_{S}[1][2]$ and $T_{S}[1][3]$ in order to find the
maximum from block 1 to 3.  In addition, our implementation will also
``peek'' into the two neighboring elements $T_{S}[1][1]$ and
$T_{S}[1][4]$, which are very likely also present in cache because
they are in the same level of the sparse table.  If the maximum from
the block maximum, i.e., the maximum of $T_{S}[1][2]$ and
$T_{S}[1][3]$, is already larger than the overall block maximum where
the prefix belongs, i.e., in $T_{S}[1][4]$, then we do not need to
check the $P$ table.  Similarly if If the maximum from the block
maximum and the prefix maximum is larger than the overall block maximum
where the suffix belongs, i.e., in $T_{S}[1][1]$, then we do not need
to check the $S$ table either.  Note that we check for the prefix
before the suffix since the value of the dynamic programming table is
{\em increasing}, so it is more likely that a prefix maximum can save a
check into the suffix table.




  
In our application, we even predict whether the Cartesian tree is
necessary to use for the in-block query. If not, we can reduce time to
compute it. These arrays need extra $O(n)$ space, but improve the
performance on lookup operation.

Algorithm~\ref{alg:rmq-access-order-2e} shows the access order for the
ranged maximum query to reduce the cache miss.  The first level of the
compressed sparse table $T_s[0]$ consider as a small cache which can
provide a probabilistic test to determine whether the load the value
from prefix/suffix maximum array is necessary.

\input{\AlgoPath/alg-rmq-access-order-2e}

