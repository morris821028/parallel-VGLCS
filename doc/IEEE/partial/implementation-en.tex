\section{Implementation} \label{sec:Implementation}

\subsection{The Strategy of Disjoint Set Implementaion}

%% There two techniques to efficiently merge two disjoint sets -- {\em
%%   path compression}, and {\em rank strategy}.

\subsubsection{Cache Performance}

The cache is essential to efficient implementation of disjoint sets.
Patwary, Blair, and Manne~\cite{Patwary2010ExperimentsOU} conducted
experiments on disjoint-set and showed that different implementations
have different impact on different levels of caches misses.  In
practice the cache miss is strongly related to how we go from a child
to its ancestors through pointer chasing during path compression.
Usually, an algorithm with a lower time complexity, e.g.,  will have more
``long jumps'' than an algorithm with a higher time complexity.  Here
the long jump means the pointer chasing will go from a memory address
to a far away memory address.  Please refer to
Figure~\ref{fig:long-short-jump-disjoint} for an illustration.

\begin{figure}[!thb]
  \centering \subfigure[An algorithm with a lower time complexity] {
    \includegraphics[width=0.42\linewidth]{\GraphicPath/fig-rem-long-jump.pdf}
  } \subfigure[An algorithm with a higher time complexity] {
    \includegraphics[width=0.42\linewidth]{\GraphicPath/fig-rem-short-jump.pdf}
  }
  \caption{The parent jump in disjoint set}
  \label{fig:long-short-jump-disjoint}
\end{figure}

% what is the full name for RemSP? What does it mean???

The Rem's algorithm ({\sc Rem})~\cite{dijkstra1976a} achieves better
cache perfromance by a {\em merge-by-index} technique.  Traditional
disjoint set merging techniqes are {\em merge-by-rank}\cite{XXX} and
{\em merge-by-size}\cite{XXX}, which determine the root of new tree by
the ranks and the sizes of the two trees respeitively.  Despite that
they do have asymtotivcally better time complexity, put their better
time complexity here.  However, their perfromance in practice is not
so imperssive due to the previously mentioned cache issue.  In
contrast Rem's algorithm assigns an {\em index} to each node, and
merge two disjoint sets according the index of the roots of the two
trees being merged.  That is, the root of the new merged tree will be
the root with the ``larger'' index.

In our experiments we still use merge-by-rank to merger trees.
However, when we have two equally ranked disjoint set trees to merger,
we apply the merge-by-index technique to resolve the case, so as to
improve cache performance.

\subsubsection{Application on VGLCS}

% VGLCS

In the serial algorithm, we tend to actual append the new elements until
appearing the non-zero elements which is the lowest value of the integer
type in our application.  It causes less jumps and modification of the
parent for each node to improve the performance.

For the multi-core platform, the efficiency of thread synchronization is
the important part of the performance, so we tends to make the worst
case as more smaller as possible.  Therefore, we always merge the nodes
as possible opposite to serial algorithm.


% I cannot even undertsand this, even it is in Chinese

\iffalse
在單一處理器下，由於動態規劃常會遇到不合定義而填入連續的 0，
多次的插入操作可以直到下一個非零的時候再進行，同時也改善查表的花費，
直到下一個非零的才進行的操作，增加嚴重增加某一次操作的時間。

在多核心平台下，要避免單一操作時間過長，一旦單一操作時間過長，
多個工作的同步將變得非常沒有效率。因此，每一次操作都強制合併，這有別於循序算法的版本。
\fi

\subsection{Parallel Range Query in VGLCS Problem}

%???

In the VGLCS problem, the information of range query can be reused a
lots of times.  We can reduce the amount of computation for our dynamic
programming problem by remove duplicate computation and imposing a
boundary limitation.  For example, the logarithm function is often used
in the query of a sparse table, and we can preprocessing all the result
of the requirements.  Therefore, the reduce-boundary Algorithm~\ref{alg
:reduce-boundary} runs $O(n \log n)$ time, which $n$  is the length of
the input sequence.  It would not increase the time complexity because
the VGLCS problem is solved in  $O(n^2 / p + n \log n)$, which $p$ is
the number of the processors.


\iffalse
在 VGLCS 問題中，平行區間詢問的每個區間範圍都是已知的，每一個會重複使用好幾次，
由於已知詢問的區間資訊，我們可以藉由建表範圍縮小，
從算法 \ref{alg:reduce-boundary} 推導邊界來減少計算量。
\fi

\input{\AlgoPath/alg-reduce-boundary-2e}

The in-block query is a very small probability event due to small $s =
\frac{\log n}{4}$. We can use prefix and suffix maximum array to reduce
the number of lookup operation on LCA table.  Please refer to
Figure~\ref{fig:compressed-sp-opt} for an illustration.

\begin{figure}[!thb]
  \centering
  \includegraphics[width=0.75\linewidth]{\GraphicPath/fig-compressed-sp-prefix-suffix.pdf}
  \caption{The prefix/suffix maximum array for each block of compressed sparse table}
  \label{fig:compressed-sp-opt}
\end{figure}

In our application, we even predict whether the Cartesian tree is
necessary to use for the in-block query. If not, we can reduce time to
compute it. These arrays need extra $O(n)$ space, but improve the
performance on lookup operation.  

Algorithm~\ref{alg:rmq-access-order-2e} shows the access order for the
ranged maximum query to reduce the cache miss.  The first level of the
compressed sparse table $T_s[0]$ consider as a small cache which can
provide a probabilistic test to determine whether the load the value
from prefix/suffix maximum array is necessary.

\input{\AlgoPath/alg-rmq-access-order-2e}
