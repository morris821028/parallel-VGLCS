\section{Implementation}
\label{sec:Implementation}

\subsection{The Strategy of Disjoint Set}

There two methods to merge disjoint sets -- {\em path compression}, and
{\em rank strategy}.  Patwary, Blair, and
Manne~\cite{Patwary2010ExperimentsOU} conducted experiments on disjoint-
set, and showed that different implementations have different impact on
different levels of caches miss, such as the lower time complexity will
need the less long jumps opposite to higher time complexity use more
short jumps. % like what???

\begin{figure}[!thb]
  \centering \subfigure[Other strategy] {
    \includegraphics[width=0.42\linewidth]{\GraphicPath/fig-rem-long-jump.pdf}
  } \subfigure[Rem's algorithm] {
    \includegraphics[width=0.42\linewidth]{\GraphicPath/fig-rem-short-jump.pdf}
  }
  \caption{The parent jump in disjoint set}
  \label{fig:long-short-jump-disjoint}
\end{figure}

% what is the full name for RemSP? What does it mean???

The Rem's algorithm ({\sc Rem})~\cite{dijkstra1976a} is the {\sc Int}
algorithm which assumes that each node has a unique identifier that can
be ordered.  The identifier often used to express $0$ to $n$.  When
union two disjoint-set trees, we make the lower index of the root node
point to higher index of the root node. Another compression technique is
known as {\sc Splicing} ({\sc Sp}) which makes the index of new parent
of each node is higher than the index of old parent after {\sc Union}
operations.

\begin{figure}[!thb]
  \centering \subfigure[Before {\sc Union} operation] {
    \includegraphics[width=0.42\linewidth]{\GraphicPath/fig-remsp-before.pdf}
  } \subfigure[After {\sc Union} operation] {
    \includegraphics[width=0.42\linewidth]{\GraphicPath/fig-remsp-after.pdf}
  }
  \caption{The {\sc RemSp} strategy}
  \label{fig:RemSp-strategy}
\end{figure}


The {\sc RemSp} is the strategy which integrates {\sc Rem} and {\sc Sp}
techniques.  The time complexity of the {\sc RemSp} is $O(m \log_{2+m/n}
n)$.  The report of Patwary et al.~\cite{Patwary2010ExperimentsOU} shows
that {\sc RemSp} is given both fewer cache missed and fewer parent jumps
than the other classical algorithms.

Recalled VGLCS problem, we consider above cache issue to implement our
program carefully.  We maintain the stack which stores the indices of
maximum value node for the {\em incremental suffix maximum query}
problem to reduce the amount of parent jumps~\cite{Peng2011TheLC} on {\sc
Append} operation.  Then, the all operations is supported by {\sc Rem}
technique.

In the serial algorithm, we tend to actual append the new elements until
appearing the non-zero elements which is the lowest value of the integer
type in our application.  It causes less jumps and modification of the
parent for each node to improve the performance.

% what is lazy propagation???

% Due to the tendency of the dynamic programming, there are two cases in
% which the trend of the inserted value. The first case is the
% continuous zero value insert because it violates the definition.

% The second case is the insertion of incremental elements.  Finally, we
% can use the lazy propagation to improve the performance in
% implementation.

For the multi-core platform, the efficiency of thread synchronization is
the important part of the performance, so we tends to make the worst
case as more smaller as possible.  Therefore, we always merge the nodes
as possible opposite to serial algorithm.


% I cannot even undertsand this, even it is in Chinese

\iffalse
在單一處理器下，由於動態規劃常會遇到不合定義而填入連續的 0，
多次的插入操作可以直到下一個非零的時候再進行，同時也改善查表的花費，
直到下一個非零的才進行的操作，增加嚴重增加某一次操作的時間。

在多核心平台下，要避免單一操作時間過長，一旦單一操作時間過長，
多個工作的同步將變得非常沒有效率。因此，每一次操作都強制合併，這有別於循序算法的版本。
\fi

\subsection{Parallel Range Query}

%???

In the VGLCS problem, the information of range query can be reused a
lots of times.  We can reduce the amount of computation for our dynamic
programming problem by remove duplicate computation and imposing a
boundary limitation.  For example, the logarithm function is often used
in the query of a sparse table, and we can preprocessing all the result
of the requirements.  Therefore, the reduce-boundary algorithm~\ref{alg
:reduce-boundary} runs $O(n \log n)$ time, which $n$  is the length of
the input sequence.  It would not increase the time complexity because
the VGLCS problem is solved in  $O(n^2 / p + n \log n)$, which $p$ is
the number of the processors.


\iffalse
運行區間查找時，一般依賴內建函數在 $O(1)$ 時間完成對數取整，
然而，在 VGLCS 這類型的動態規劃中，區間查找的對數結果是可以被預測的，預先將每一組詢問的區段對數結果儲存在陣列中，便可降低指令次數。
\fi

\iffalse
由於已知所有詢問區間，建立稀疏表時，可藉由動態規劃在 $O(n \log n)$ 排除掉不可能的計算 (參照算法 ~\ref{alg:reduce-boundary})，
降低過程中的計算量。由於 VGLCS 在平行操作需要 $O(n \log n)$，故使用動態規劃不影響我們的最終結果。
\fi

\input{\AlgoPath/alg-reduce-boundary-2e}

Due to small $s = \frac{\log n}{4}$, the in-block query is a very
small probability event.  We can use prefix and suffix maximum array
to instead of the lookup table.  In our application, we even predict
whether the Cartesian tree is necessary to use for the in-block query.
If not, we can reduce time to compute it.  These two arrays brought
$O(n)$ space, but improve the performance by strength reduction.

\iffalse
從機率分佈的角度來看，因 $s = \frac{1}{4} \log n$ 過小，區間詢問完全落於 block 的機率低，
故額外維護區段前綴和後綴最大值 (prefix/suffix maximum value in block) 取代笛卡爾樹的建立。
藉這兩個額外儲存空間，將會增加空間複雜度的常數，卻能有效地降低整體的指令次數。
\fi
