\section{Implementation Optimizations} \label{sec:Implementation}

This section describes the optimization in our implmentations of VGLCS
algorithms, both in seqential and parrallel environments.  Note that
some of these techniques address hardware characteristics, e.g., cache
bahavior, and are not confined to asymtotical analyasis only.

\subsection{The Strategy of Disjoint Set Implementaion}

We now describe the optimization in the implmentations of disjoint
sets, whose applications include VGLCS.

%% There two techniques to efficiently merge two disjoint sets -- {\em
%%   path compression}, and {\em rank strategy}.

\subsubsection{Cache Performance}

The cache is essential to efficient implementation of disjoint sets.
Patwary, Blair, and Manne~\cite{Patwary2010ExperimentsOU} conducted
experiments on disjoint-set and showed that different implementations
have different impact on different levels of caches misses.  In
practice the cache miss is strongly related to how we go from a child
to its ancestors through pointer chasing during path compression.
Usually, an algorithm with a lower time complexity, e.g.,  will have more
``long jumps'' than an algorithm with a higher time complexity.  Here
the long jump means the pointer chasing will go from a memory address
to a far away memory address.  Please refer to
Figure~\ref{fig:long-short-jump-disjoint} for an illustration.

\begin{figure}[!thb]
  \centering \subfigure[An algorithm with a lower time complexity] {
    \includegraphics[width=0.42\linewidth]{\GraphicPath/fig-rem-long-jump.pdf}
  } \subfigure[An algorithm with a higher time complexity] {
    \includegraphics[width=0.42\linewidth]{\GraphicPath/fig-rem-short-jump.pdf}
  }
  \caption{The parent jump in disjoint set}
  \label{fig:long-short-jump-disjoint}
\end{figure}

% what is the full name for RemSP? What does it mean???

The Rem's algorithm ({\sc Rem})~\cite{dijkstra1976a} achieves better
cache perfromance by a {\em merge-by-index} technique.  Traditional
disjoint set merging techniqes are {\em merge-by-rank}\cite{XXX} and
{\em merge-by-size}\cite{XXX}, which determine the root of new tree by
the ranks and the sizes of the two trees respeitively.  Despite that
they do have asymtotivcally better time complexity, put their better
time complexity here.  However, their perfromance in practice is not
so imperssive due to the previously mentioned cache issue.  In
contrast Rem's algorithm assigns an {\em index} to each node, and
merge two disjoint sets according the index of the roots of the two
trees being merged.  That is, the root of the new merged tree will be
the root with the ``larger'' index.

In our experiments we still use merge-by-rank to merger trees.
However, when we have two equally ranked disjoint set trees to merger,
we apply the merge-by-index technique to resolve the case, so as to
improve cache performance.

\subsubsection{Application on VGLCS}

% VGLCS

It is possible to further optimize the implementation of Peng's
sequential algorithm (Algorithm~\ref{alg:serial-VGLCS}) by a {\em lazy
  insertion} technique.  Recall that in
Algorithm~\ref{alg:serial-VGLCS} when the $i$-th characters from one
input string does not match the $j$-th character of the other input
string, we need to place a zero into the dynamic programming table at
position $V[i][j]$.  In practice this this mismatch will happen very
frequently, and causing frequent insertions of zeros into the disjoint
sets.  Each insertion then links the newly inserted singlton node of
zero the previous node of zero that was inserted one iteration ago. 

We implemented an optimization that resolves this repeated insertion,
and linking, of zeros.  Our implementation will scan through a series
of zeros, and locate the next non-zero $V$ element (denoted as $v$) in
the same column, and insert them as a batch.  Since all values in $V$
are non-negative, and hence larger than zero, we can link all zeros to
$v$.  From the experiments we observe that this technique causes less
pointer chasing and updates and does improve performance.

Note that we do not apply the lazy insertion optimization in our
parallel implementation.  For a multi-core platform, the efficiency of
thread synchronization is essential to the performance.  Since the
threads of a parallel VGLCS algorithm needs to synchronize at every
column of the dynamic programming table in order to process a row, the
lazy insertion, which can ingore this dependency in a sequential
environment, is not beneficial, and is not adopted in our parallel
implementation.

\subsection{Parallel Range Query in VGLCS}

%% %???

%% In the VGLCS problem, the information of range query can be reused a
%% lots of times.  We can reduce the amount of computation for our
%% dynamic programming problem by remove duplicate computation and
%% imposing a boundary limitation.  For example, the logarithm function
%% is often used in the query of a sparse table, and we can preprocessing
%% all the result of the requirements.  Therefore, the reduce-boundary
%% Algorithm~\ref{alg:reduce-boundary} runs $O(n \log n)$ time, which $n$
%% is the length of the input sequence.  It would not increase the time
%% complexity because the VGLCS problem is solved in $O(n^2 / p + n \log
%% n)$, which $p$ is the number of the processors.

%% \iffalse
%% 在 VGLCS 問題中，平行區間詢問的每個區間範圍都是已知的，每一個會重複使用好幾次，
%% 由於已知詢問的區間資訊，我們可以藉由建表範圍縮小，
%% 從算法 \ref{alg:reduce-boundary} 推導邊界來減少計算量。
%% \fi

%% \input{\AlgoPath/alg-reduce-boundary-2e}

The in-block query is a very small probability event due to small $s =
\frac{\log n}{4}$. We can use prefix and suffix maximum array to
reduce the number of lookup operation on LCA table.  Please refer to
Figure~\ref{fig:compressed-sp-opt} for an illustration.

\begin{figure}[!thb]
  \centering
  \includegraphics[width=0.75\linewidth]{\GraphicPath/fig-compressed-sp-prefix-suffix.pdf}
  \caption{The prefix/suffix maximum array for each block of
    compressed sparse table}
  \label{fig:compressed-sp-opt}
\end{figure}

In our application, we even predict whether the Cartesian tree is
necessary to use for the in-block query. If not, we can reduce time to
compute it. These arrays need extra $O(n)$ space, but improve the
performance on lookup operation.

Algorithm~\ref{alg:rmq-access-order-2e} shows the access order for the
ranged maximum query to reduce the cache miss.  The first level of the
compressed sparse table $T_s[0]$ consider as a small cache which can
provide a probabilistic test to determine whether the load the value
from prefix/suffix maximum array is necessary.

\input{\AlgoPath/alg-rmq-access-order-2e}

