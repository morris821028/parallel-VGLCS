\section{Experiment}
\label{sec:Experiment}

We run on Intel Xeon E5-2620 2.40 GHz server, which the size of L1
cache is 384 KiB, the size of L2 is 1536 KiB, and the size of L3 cache
is 15 MiB and have two processors.  The program is implemented with
C++ language and OpenMP. The compile flag is \texttt{-O2}.  The CPU
supports hyper threading technique, and each processors has six cores.

\subsection{VGLCS with Compressed Sparse Table}

In the random test data, we get the better perfromance with compressed
sparse table compared to theoretical $\theta(1)$ sparse table.  The
figure \ref{fig:fig-parallel} shows the different runtime in different
parallelism, and the figure \ref{fig:fig-paralell} shows the
scalability of our parallel algorithm.  On our server, the parallel
algorithm can get $8 \times$ faster than serial algorithm.

\begin{figure*}[!thb]
  \centering
  \subfigure[Runtime]{
    \includegraphics[width=0.45\linewidth]{graphics/fig-parallel-n.pdf}
    \label{fig:fig-parallel}
  }
  \subfigure[Scalability]{
    \includegraphics[width=0.45\linewidth]{graphics/fig-parallel-p.pdf}
    \label{fig:fig-parallel-scala}
  }
  \caption{Serial and Parallel Algorithm run on E5-2620}
\end{figure*}

%\subsection{理論常數 VGLCS}

%尚未完成

\subsection{Incremental Suffix Maximum Query}

In VGLCS problem, the number of insertion equals the number of query.
We compare the performance between the four data structure as follows:

\begin{itemize}
  \item 

Disjoint Set: Average run time $o(\alpha(n))$.  It is implemented by
path compression.

  \item 

Sparse Table: Insertion in $O(\log n)$ time, and query in $O(1)$ time.
We allocate $\tt{table}[\log N][N]$ which arranged in row majar to
reduce cache-miss.

  \item 

Binary Indexed Tree: Both insert and query operation consume $O(\log
n)$ time.

  \item 

Compressed Sparse Tree: The insertion operation consume amortized
$O(1)$ time.  The query operation consume $O(s)$ time, which $s$ is
the size of block.  We can maitain extra the prefix and suffix maximum
in blcok to reduce the time complexity $O(s)$ to $O(1)$ in most
queries.

\end{itemize}

The figure \ref{fig:fig-ISMQcmp} shows that our compressed sparse
table run faster than the disjoint set when $n > 10^6$.  When $n >
10^7$, the compressed sparse table run faster $1.25 \times$ than the
disjoint set.  

\begin{figure}[!thb]
  \centering
  \includegraphics[width=0.9\linewidth]{graphics/fig-ISMQ.pdf}
  \caption{ISMQ runs on E5-2620 with different data structures. We use the random test cases without any limitation, e.g. the length of interval query has a uniform distribution.}
  \label{fig:fig-ISMQcmp}
\end{figure}

However, we observe that our amortized $\theta(1)$ sparse table is more
slow than  disjoint set.  In the dynamic programming, the trendency of insertion value will inflict the performance of data structure, so we experiment the different probability for each data structure.  The nubmer of query is 10 multiple of the number of insertion.  The result is shown on table \ref{tlb:ISMQcmp}.  When $N=10^7$, our amortized $\theta(1)$ sparse table run faster $1.26 \times$ than the disjoint set.

\input{./tables/tlb-ISMQcmp.tex}

\subsection{Parallel Range Query}

Each stage will have $n$ number of elements and the $n$ number of query.  In this special cases, we run the \texttt{CORMQ} (compressed RMQ) to improve the performance compared to the origin.  If we can predict all range size before building, we get the \texttt{CORMQ-opt} which run $2.35$ faster than origin RMQ.  The table \ref{tlb:CORMQ} is shown the result of the strategy of parallel range query.

\input{./tables/tlb-CORMQ.tex}