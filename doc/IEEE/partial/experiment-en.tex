\section{Experiment}
\label{sec:Experiment}

We conduct experiments on a Intel Xeon E5-2620 2.4 Ghz processor with
384K bytes of level 1 cache, 1536K bytes of level 2 cache, and 15M
bytes of shared level 3 cache.  The Intel CPU supports
hyper-threading, and each processor has six cores.  The opearating
system is Ubuntu 14.04.  We implemented all algorithms in C++ and
OpenMP and compiled them using gcc with {\tt -O2} and {\tt -fopenmp}
flag.

We implement {\em sparse table} and {\em binary index tree} in our
experiments and evaluate their performance.  Recall that there are
three data structures we can use to answer incremental suffix maximum
query.  The first one is {\em disjoint set}, and it has an amortized
time $O(\alpha(n))$ for each query.  Its variant, {\em incremental
  tree set union}~\cite{Gabow1983ALA}, has an amortized time $O(1)$
for each query.  The second data structure is {\em sparse table}.  It
takes $O(\log n)$ time to append a new value and $O(1)$ time to answer
a query.  The third data structure is {\em binary indexed tree}.  It
takes $O(\log n)$ time to answer a query.  Although the amortized
complexity of answering a query with a disjoint set is $O(\alpha(n))$
or $O(1)$, it causes inefficient synchronizations in a parallel
environment, therefore, we chose to implement the last two data
structures and conduct experiments to evaluate their performance.

\subsection{VGLCS with Compressed Sparse Table}

In the random test data, we get the better performance with compressed
sparse table compared to theoretical $O(1)$ sparse table.  The
figure~\ref{fig:fig-parallel} shows the different runtime in different
parallelism, and the figure~\ref{fig:fig-parallel-scala} shows the
scalability of our parallel algorithm.  On our server, the parallel
algorithm can get $8 \times$ faster than the serial algorithm.

\iffalse
我們運行優化策略中的空間壓縮版本，而非理論分析的 $\theta(1)$ 操作，
單次詢問落在 $O(s)$ 中，在實作上由於可以完全壓在暫存器上操作，效能表現較佳。
\fi

\begin{figure}
  \centering
  \subfigure[Runtime]{
    \includegraphics[width=0.45\linewidth]{\GraphicPath/fig-parallel-n.pdf}
    \label{fig:fig-parallel}
  }
  \subfigure[Scalability]{
    \includegraphics[width=0.45\linewidth]{\GraphicPath/fig-parallel-p.pdf}
    \label{fig:fig-parallel-scala}
  }
  \caption{Serial and Parallel Algorithm run on E5-2620}
\end{figure}

%\subsection{理論常數 VGLCS}

%尚未完成

\subsection{Incremental Suffix Maximum Query}

In VGLCS problem, the number of insertion equals the number of the
queries. We compare the performance between the four data structure as
follows:

\iffalse
針對插入和詢問次數相同的 ISMQ 問題，運行以下四種數據結構：
\fi

\begin{itemize}
  \item 

Disjoint set: Average run time $o(\alpha(n))$ which is implemented by
path compression and rank.

  \item 

Sparse table: Insertion in $O(\log n)$ time, and query in $O(1)$ time.
We allocate $\tt{table}[\log N][N]$ which arranged in row-major to
reduce cache miss.

%   \item 

% Binary indexed tree: Both insert and query operation consume $O(\log
% n)$ time.

  \item 

Compressed sparse table: The insert operation consume amortized
$O(1)$ time.  The query operation consumes $O(s)$ time, which $s$ is the
size of the block.  Here, the program implement by the fixed size $s =
16$.  We can maintain extra the prefix and suffix maximum in block to
reduce the time complexity $O(s)$ to $O(1)$ in most queries.

  \item

Amortized $O(1)$ sparse table:  The insert operation consume amortized
$O(1)$ time.  The query operation consume $O(1)$ time.  In the
implementation, we choose the fixed size $s = 8$.  The other
optimization is same as compressed sparse table.

\end{itemize}

\iffalse
\begin{itemize}
  \item 并查集 (Disjoint Set): 平均運行時間 $o(\alpha(n))$。只使用路徑壓縮技巧。
  \item 稀疏表 (Sparse Table): 插入 $O(\log n)$、詢問 $O(1)$。實作陣列宣告採用 $\tt{table}[\log N][N]$ 以減少快取未中。
  \item 樹狀數組 (Binary Indexed Tree): 插入、詢問均為 $O(\log n)$。
  \item 壓縮稀疏表 (Compressed Sparse Tree): 插入均攤 $O(1)$、詢問操作 $O(s)$，
  其中 $s$ 為拆分到區塊大小。實作時，維護區塊前綴和後綴最大值降低詢問複雜度至 $O(1)$，當發生 in-block 詢問再運行 $O(s)$ 算法。
\end{itemize}
\fi

The figure~\ref{fig:fig-ISMQcmp} shows that our compressed sparse table
run faster than the disjoint set when $n$ is greater than  $10^6$.  When
$n$ is greater than $10^7$, the compressed sparse table run $1.8 \times$
faster than the disjoint set in random test data.  We use the
random test cases without any limitation, e.g. the length of range
query has a uniform distribution.

\begin{figure}[!thb]
  \centering
  \includegraphics[width=\linewidth]{\GraphicPath/fig-ISMQ.pdf}
  \caption{
  The performance of the ISMQ problem with different data structures on 
  E5-2620 server.
  }
  \label{fig:fig-ISMQcmp}
\end{figure}

However, we observe that our amortized $O(1)$ sparse table is more
slow than compressed sparse table.  In the dynamic programming, the
tendency of insertion value will inflict the performance of data
structure, so we experiment the different probability for each data
structure.  The number of the query is ten multiple of the number of
insertion.  The result is shown on table~\ref{tlb:ISMQcmp}.  The
table~\ref{tlb:ISMQcmp} show the performance of the ISMQ with sizes $N
= 10^7$, different maximum interval sizes $L$, the probability $p$ of
incremental elements and the probability $q$ of zero elements. There
are two strategies for each $(p, q, L)$, implemented by compressed
sparse table and amortized $O(1)$ sparse table.  The amortized $O(1)$
sparse table run $1.5 \times$ faster than the compressed table in
above situation.

\iffalse 當運行 $n > 10^6$ 時，我們提出的壓縮稀疏表的效能已經勝過并查
集的版本，其運行結果如圖表 ~\ref{fig:fig-ISMQcmp}。在 $n = 10^7$ 時，
加速 $1.25 \times$。然而，我們提供的 amortized $\theta(1)$ 的稀疏表慢
於并查集，我們做了深入的機率探討 (參照表 ~\ref{tlb:ISMQcmp})，由於大部
分的操作都被區塊後綴和前綴解決，沒有實際運用到內部詢問，約束區間詢問的
大小為 $L$，在 $N = 10^7$ 時，最多能加速 $1.26 \times$，其中插入和詢問
比例為 1:10，當詢問比重更大時，將有更明顯的加速。\fi

\input{\TablePath/tlb-ISMQcmp.tex}

\subsection{Parallel Range Query}

Each stage will have $n$ number of elements and the $n$ number of the
query.  In this special cases, we run the {\tt CORMQ} (compressed RMQ)
to improve the performance compared to the origin.  If we can predict
all range size before building, we get the {\tt CORMQ-opt} which run
$2.35$ faster than origin RMQ.  The table~\ref{tlb:CORMQ} is shown the
result of the strategy of the parallel range query.

\iffalse 每一次有 $n$ 個元素和 $n$ 組詢問，針對這種特殊性質的問題，我
們運行樸素的 \texttt{CORMQ} (compressed RMQ) 得到效能改善，搭配可預測
的分析降低運算量 (參照 \texttt{CORMQ-opt})，得到更好的改善。在
\texttt{CORMQ-opt} 策略中，得到 $2.35 \times$ 倍的加速，結果如表
~\ref{tlb:CORMQ}。\fi

\input{\TablePath/tlb-CORMQ.tex}
