\section{Experiment} \label{sec:Experiment}

We conduct three sets of experiments.  The first set compares the
performance of blocked and unblocked spare tables in supporting parallel
range maximum query.  The second set evaluates the performance of
incremental suffix maximum query using various data structures,
especially the blocked sparse tables.  Finally, the third experiment
evaluates the effects of different data structures on the overall
performance of VGLCS computation.

We conduct experiments on an Intel Xeon E5-2620 2.4 Ghz processor with
384K bytes of level 1 cache, 1536K bytes of level 2 cache, and 15M
bytes of shared level 3 cache.  The Intel CPU supports
hyper-threading, and each processor has six cores.  The operating
system is Ubuntu 14.04.  We implemented all algorithms in C++ and
OpenMP and compiled them using gcc with {\tt -O2} and {\tt -fopenmp}
flag.

\subsection{Blocked and Unblocked Sparse Table}

We first compare the performance of {\em unblocked} sparse table
(Section~\ref{sec:sparse-table}) and {\em blocked} sparse tables
(Section~\ref{sec:blocked-sparse-table}) for supporting parallel range
maximum query.  For the blocked sparse table, we use the
rightmost-pops encoding instead of the LCA tables, since we will show
that the rightmost-pops encoding is more efficient than LCA table in
the next set of experiments.  We test all possible query range sizes
and the number of range queries is set to be the number of elements
$N$.

Table~\ref{tlb:CORMQ} compares the parallel range query time of
unblocked sparse table and blocked sparse table, and we observe that
the blocked sparse table using rightmost-pops sparse table is more
efficient than the unblocked sparse table when $N$ increases.  For
example, the blocked sparse table is $1.4$ times faster than the
unblocked sparse table when $N$ reaches $100000$.

We believe that the maximum length of interval query ($L$ in
Table~\ref{tlb:CORMQ}) affects the cache performance of accessing a
sparse table.  In addition, even when we fix the number of data $N$,
the speedup of blocked sparse table over unblocked one does increase
as $L$ increases.  We believe that the reason of this speedup is as
follows.  Even with a very large query length, a blocked sparse table
will only need to access a limited range of memory.  For example, the
rightmost-pops encoding only needs to access two elements that are
$\log {N/s}$ apart in memory (the sparse table for the block maximums)
for super block query, and two in-block queries $O(s)$ in consecutive
memory.  In contrast, the unblocked sparse table can have up to $\log
N$ levels, and different queries of different sizes will need to
access different levels of the sparse table.  This does not help cache
locality because consecutive queries will unlikely to access
consecutive memory.  When $N$ is small this will not be problem for
unblocked sparse table since its size will be small.  When $N$
increases the cache locality of blocked sparse table becomes more
significant.

% ($t_i$ the number of pops) 

\input{\TablePath/tlb-CORMQ.tex}

\subsection{Rightmost-pops and LCA Tables}

We now compare the performance of {\em four} data structures for
supporting {\em incremental suffix} query.  The four data structures
are {\em disjoint set}, {\em unblocked} sparse table, blocked table
with {\em rightmost-pops encoding}, and blocked sparse table with
LCA. Note that since we are testing suffix queries so we can use
disjoint set.  Please refer to Section~\ref{sec:parallelRMQ} for
details. For disjoint set, we implemented both {\em path compression}
and {\em merge-by-rank} strategies so that the amortized time
$O(\alpha(n))$. In our implementation of blocked sparse tables, we set
the block size $s$ to $8$.  All sparse tables are allocated in memory
in a row-major manner to reduce cache miss.  Please refer to
Algorithm~\ref{alg:parallel-VGLCS} and
Figure~\ref{fig:interval-decomposition} for details.

% Here we need a table of all complexity.

We first tried a simple scenario in which we alternate between {\em
  appending} a data and {\em querying} a range.  Both the length and
the position of the range query are uniformly distributed among all
possibilities.  Figure~\ref{fig:fig-ISMQcmp} shows the performance of
the four data structures for supporting incremental suffix maximum
queries under this simple scenario.  We note that our rightmost-pops
encoding implementation runs faster than all other
implementations. For example, it runs faster than the disjoint set
implementation when the number of data $n$ reaches $10^6$.  In
particular, when $n$ is greater than $10^7$, the rightmost-pops
encoding runs $1.8$ times faster than the disjoint set.


\begin{figure}[!thb]
  \centering
  \includegraphics[width=0.75\linewidth]{\GraphicPath/fig-ISMQ.pdf}
  \caption{The performance of the different data structures for
    supporting incremental suffix maximum query on an E5-2620 server}
  \label{fig:fig-ISMQcmp}
\end{figure}

Now we conduct another experiment in a more complex scenario.  In a
dynamic programming various factors affect performance, these factors
include the distribution of values being inserted, the maximum
interval being queried, and the ratio between the numbers of insertion
and queries.  For ease of notation, we use $p$ to denote the
probability of insert a larger next element, $q$ for the probability
of inserting a zero, and $L$ for the maximum interval length being
queried.  In our experiments, we set the number of data $N$ to be
$10^7$, and vary the maximum interval sizes $L$ from 4 to 16, and vary
$p$ and $q$ from 0 to 100\%.  We also set the number of the queries to
be {\em ten} times of the number of insertions.

From Figure~\ref{fig:fig-ISMQcmp}, we observe that blocked sparse
table implementations outperform disjoint set and unblocked sparse
table, so we will focus on comparing the two implementations for
blocked sparse table, i.e., rightmost-pops and LCA tables.  Note that
it is easy to extend rightmost-pops encoding for incremental data
since we only need to maintain the number of pops incrementally for
the last block.

Table~\ref{tlb:ISMQcmp} compares the time for answering incremental
suffix maximum query using rightmost-pops and LCA tables.  Despite
that the LCA table method has a theoretically better amortized $O(1)$
query time, we observe that the rightmost-pops runs up to $1.5$ times
faster than the LCA table.  We believe that there are two reasons for
this.  First, the LCA table implementation requires more instructions
to compute the Catalan index.  In contrast, the rightmost-pops
encoding does not require the computation for tree index.  Instead it
uses the stored number of pops to answer the query directly.  Second,
despite that in Theory~\cite{Fischer2006TheoreticalAP} the optimal
block size is $\frac{\log n}{4}$, this block size is usually too large
for LCA table implementations.  That is, we will need to build a very
large LCA lookup table.  Even worse, we will {\em not} access all of
it, which means it is very unlikely that we will access contiguous
memory, which causes serious cache misses.


\input{\TablePath/tlb-ISMQcmp-new.tex}

We also observe that the probability $p$ affects the performance gain
of our ``peeking'' technique.  When the probability $p$ is close to 0
or 1, the peeking achieves excellent performance gain.  The
performance gain of the peeking operation also depends the block size.
In addition, the best block size for LCA sparse table is different
from the best block size for the rightmost-pops sparse table.

We observe that the performance of the rightmost-pops sparse table is
better than that of LCA sparse table, but the performance gain
decreases when $q$ is close to 1.  LCA sparse table requires the
Cartesian index computation, which is not required in the
rightmost-pops sparse table implementation.  When the probability $q$
is close to 1, LCA sparse table will require less instructions on
computing Cartesian index, so its performance will be close to that of
the the rightmost-pops sparse table.

The maximum length of interval query ($L$ in Table~\ref{tlb:ISMQcmp})
affects the performance of interval query in a rightmost-pops sparse
table, but not the performance of the LCA sparse table.  Note that in
Algorithm~\ref{alg:cartesian64bits-query} the block size $s$ is set to
16, which is the maximum number of times the loop in
Algorithm~\ref{alg:cartesian64bits-query} will iterate for an query.
Therefore, the query time will increase when $L$ increases.  On the
other hand, the LCA sparse table answers a query by a single lookup
into the LCA table, so its performance is not affected by $L$.

\subsection{Variable Gapped Longest Common Subsequence}

We now compare all four data structures in the final experiments and
evaluate their overall performance on VGLCS.  We implemented {\em
  four} combinations of data structures in solving the VGLCS problem
and evaluated their performance.  The first combination is a {\em
  sequential} implementation of Peng's algorithm using disjoint set on
{\em both} the first stage and second stage.  Note that since the
disjoint set only supports suffix query, so the implementation must be
sequential if the second stage uses the disjoint set.  On the other
hand, the other three combinations are all implemented in
parallel. The {\em DS-ST} combination uses disjoint set in the first
stage and the standard {\em unblocked} sparse table in the second
stage.  The {\em DS-BST} combination uses disjoint set in the first
stage and {\em blocked} sparse table with rightmost-pops encoding in
the second stage.  The reason we use rightmost-pops encoding, instead
of LCA tables, was describe earlier.  Finally, the {\em BST2}
combination uses blocked sparse table with rightmost-pops encoding in
both stages.

Figure~\ref{fig:fig-parallel} compares the execution time of all four
data structure combinations under different lengths of inputs.  The
input strings are generated {\em randomly} from the alphabet $\{A, T,
C, G\}$, as this alphabet is most popular in bioinformatics.  We note
that, {\em BST2} outperforms all other parallel implementations. % why

Figure~\ref{fig:fig-parallel-scala} shows the scalability of the best
parallel combination {\em BST2}.  It is {\em eight} times faster than
a serial implementation on our server with 6 cores and
hyper-threading.

\begin{figure}[!thb]
  \centering
  \subfigure[Execution time of four data structure combinations]{
    \includegraphics[width=0.45\linewidth]{\GraphicPath/fig-parallel-n.pdf}
    \label{fig:fig-parallel}
  }
  \subfigure[Scalability of the sparse table with rightmost-pops encoding]{
    \includegraphics[width=0.45\linewidth]{\GraphicPath/fig-parallel-p.pdf}
    \label{fig:fig-parallel-scala}
  }
  \caption{The execution time and scalability results of our parallel
    implementations on an E5-2620 server with 6 cores and
    hyper-threading}
\end{figure}
