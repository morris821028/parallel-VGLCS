\section{Experiment}
\label{sec:Experiment}

We conduct experiments on an Intel Xeon E5-2620 2.4 Ghz processor with
384K bytes of level 1 cache, 1536K bytes of level 2 cache, and 15M
bytes of shared level 3 cache.  The Intel CPU supports
hyper-threading, and each processor has six cores.  The operating
system is Ubuntu 14.04.  We implemented all algorithms in C++ and
OpenMP and compiled them using gcc with {\tt -O2} and {\tt -fopenmp}
flag.

We implement {\em disjoint set} and {\em sparse table} in our
experiments and evaluate their performance.  Recall that there are three
data structures we can use to answer incremental suffix maximum query.
The first one is {\em disjoint set}, and it has an amortized time
$O(\alpha(n))$ for each query.  Its variant, {\em incremental   tree set
union}~\cite{Gabow1983ALA}, has an amortized time $O(1)$ for each query.
The second data structure is {\em sparse table}.  It takes $O(\log n)$
time to append a new value and $O(1)$ time to answer a query.  Although
the amortized complexity of answering a query with a disjoint set is
$O(\alpha(n))$ or $O(1)$, it causes inefficient synchronizations in a
parallel environment, therefore, we chose to implement the last data
structures and conduct experiments to evaluate their performance.

\subsection{Variable Gapped Longest Common Subsequence}

We implemented four combinations of data structures in solving the
VGLCS problem and evaluated their performance.  The first combination
is a {\em sequential} implementation of Peng's algorithm using
disjoint set on {\em both} the first stage and second stage.  The
other three combinations are implemented in parallel environment.  The
{\em parallel-ST-disjoint} combination uses disjoint set in the first
stage and sparse table in the second stage.  The {\em
  parallel-COST-disjoint} combination uses disjoint set in the first
stage and compressed sparse table in the second stage.  Finally the
{\em parallel-COST} combination uses compressed sparse table in both
first and second stage.

We compare the performance of the four combinations on input strings
generated randomly from the alphabet $\{A, T, C, G\}$.
Figure~\ref{fig:fig-parallel} shows the execution time of all
combinations under different lengths of input.  We note that the
compressed sparse table implementation outperforms the theoretically
better $O(1)$ sparse table implementation.  % why?
That is, {\em parallel-COST} outperforms all other parallel implementations.

Figure~\ref{fig:fig-parallel-scala} shows the scalability of the best
parallel combination {\em parallel-COST}.  It achieves a speedup of
$8$ than the serial implementation on our server with 6 cores and
hyper-threading.

\iffalse
我們運行優化策略中的空間壓縮版本，而非理論分析的 $\theta(1)$ 操作，
單次詢問落在 $O(s)$ 中，在實作上由於可以完全壓在暫存器上操作，效能表現較佳。
\fi

\begin{figure}
  \centering
  \subfigure[Runtime]{
    \includegraphics[width=0.45\linewidth]{\GraphicPath/fig-parallel-n.pdf}
    \label{fig:fig-parallel}
  }
  \subfigure[Scalability]{
    \includegraphics[width=0.45\linewidth]{\GraphicPath/fig-parallel-p.pdf}
    \label{fig:fig-parallel-scala}
  }
  \caption{The execution time and scalability results of our parallel
    implementations on an E5-2620 server with six cores and
    hyper-threading}
\end{figure}


\subsection{Incremental Suffix Maximum Query}

We compare the performance of four data structures for supporting
incremental suffix maximum query.  The experiments alternate between
appending a data and issuing a range query.  Both the length and the
position of the range query are uniformly distributed among all
possibilities.

The first data structure is {\em disjoint set}.  We implemented path
compression and rank strategies so that the amortized time
$o(\alpha(n))$.


The second data structure is {\em sparse table}.  The insertion time
is $O(\log n)$, and the query time is $O(1)$.  The sparse table is
allocated in row-major to reduce cache miss.  Please refer to
Algorithm~\ref{alg:parallel-VGLCS} and
Figure~\ref{fig:interval-decomposition} for details.

The third data structure is {\em compressed sparse table}.  The
amortized insertion time is $O(1)$ and the query time is $O(s)$, where
$s$ is the size of the block and is set to $16$ in the experiments.

Please refer to Section~\ref{sec:parallelRMQ} for details.

The final data stricture is {\em amortized sparse table}.  Both the
amortized insertion time and the query time are $O(1)$.  In our
implementation we set the block size $s$ to $8$, and apply other
similar optimization as in compressed sparse table.

\iffalse
\begin{itemize}
  \item 并查集 (Disjoint Set): 平均運行時間 $o(\alpha(n))$。只使用路徑壓縮技巧。
  \item 稀疏表 (Sparse Table): 插入 $O(\log n)$、詢問 $O(1)$。實作陣列宣告採用 $\tt{table}[\log N][N]$ 以減少快取未中。
  \item 樹狀數組 (Binary Indexed Tree): 插入、詢問均為 $O(\log n)$。
  \item 壓縮稀疏表 (Compressed Sparse Tree): 插入均攤 $O(1)$、詢問操作 $O(s)$，
  其中 $s$ 為拆分到區塊大小。實作時，維護區塊前綴和後綴最大值降低詢問複雜度至 $O(1)$，當發生 in-block 詢問再運行 $O(s)$ 算法。
\end{itemize}
\fi

Figure~\ref{fig:fig-ISMQcmp} indicates that our compressed sparse
table implementation runs faster than the disjoint set implementation
when the number of data $n$ is greater than $10^6$.  In particular
when $n$ is greater than $10^7$, the compressed sparse table runs
$1.8$ times faster than the disjoint set.

\begin{figure}[!thb]
  \centering
  \includegraphics[width=0.45\linewidth]{\GraphicPath/fig-ISMQ.pdf}

  \caption{The performance of the different data structures for
    supporting incremental suffix maximum query on an E5-2620 server}
  \label{fig:fig-ISMQcmp}
\end{figure}

However, we observe that our amortized $O(1)$ sparse table is more
slow than compressed sparse table.  In the dynamic programming, the
tendency of insertion value will inflict the performance of data
structure, so we experiment the different probability for each data
structure.  The number of the query is ten multiple of the number of
insertion.  The result is shown on table~\ref{tlb:ISMQcmp}.  The
table~\ref{tlb:ISMQcmp} show the performance of the ISMQ with sizes $N
= 10^7$, different maximum interval sizes $L$, the probability $p$ of
incremental elements and the probability $q$ of zero elements. There
are two strategies for each $(p, q, L)$, implemented by compressed
sparse table and amortized $O(1)$ sparse table.  The amortized $O(1)$
sparse table run $1.5 \times$ faster than the compressed table in
above situation.

\iffalse 當運行 $n > 10^6$ 時，我們提出的壓縮稀疏表的效能已經勝過并查
集的版本，其運行結果如圖表 ~\ref{fig:fig-ISMQcmp}。在 $n = 10^7$ 時，
加速 $1.25 \times$。然而，我們提供的 amortized $\theta(1)$ 的稀疏表慢
於并查集，我們做了深入的機率探討 (參照表 ~\ref{tlb:ISMQcmp})，由於大部
分的操作都被區塊後綴和前綴解決，沒有實際運用到內部詢問，約束區間詢問的
大小為 $L$，在 $N = 10^7$ 時，最多能加速 $1.26 \times$，其中插入和詢問
比例為 1:10，當詢問比重更大時，將有更明顯的加速。\fi

\input{\TablePath/tlb-ISMQcmp.tex}

\subsection{Parallel Range Query}

Each stage will have $n$ number of elements and the $n$ number of the
query.  In this special cases, we run the compressed sparse table to
improve the performance compared to the origin.  If we preprocess all
range size and mark the necessary compressed blocks before building, we
get the optimized version which run $2.35$ faster than origin sparse
table. The Table~\ref{tlb:CORMQ} is shown the result of the strategy of
the parallel range query.

\iffalse 每一次有 $n$ 個元素和 $n$ 組詢問，針對這種特殊性質的問題，我
們運行樸素的 \texttt{CORMQ} (compressed RMQ) 得到效能改善，搭配可預測
的分析降低運算量 (參照 \texttt{CORMQ-opt})，得到更好的改善。在
\texttt{CORMQ-opt} 策略中，得到 $2.35 \times$ 倍的加速，結果如表
~\ref{tlb:CORMQ}。\fi

\input{\TablePath/tlb-CORMQ.tex}
