\section{Experiment} \label{sec:Experiment}

We conduct experiments on an Intel Xeon E5-2620 2.4 Ghz processor with
384K bytes of level 1 cache, 1536K bytes of level 2 cache, and 15M
bytes of shared level 3 cache.  The Intel CPU supports
hyper-threading, and each processor has six cores.  The operating
system is Ubuntu 14.04.  We implemented all algorithms in C++ and
OpenMP and compiled them using gcc with {\tt -O2} and {\tt -fopenmp}
flag.

We implement {\em disjoint set} and {\em sparse table} in our
experiments and evaluate their performance.  The disjoint set has an
amortized time $O(\alpha(n))$ for each query.  Its variant, {\em
  incremental tree set union}~\cite{Gabow1983ALA}, has an amortized
time $O(1)$ for each query.  On the other hand, the sparse table
requires $O(\log n)$ time to append a new value and $O(1)$ time to
answer a query.

We conduct experiments under two scenarios.  The first set of
experiments evaluate various data structures for supporting different
queries of VGLCS problem.  The second set of experiments purely
evaluate the performance of incremental suffix maximum query using
various data structures.  The third set evaluate the performance of
parallel range query.

\subsection{Variable Gapped Longest Common Subsequence}

We implemented four combinations of data structures in solving the
VGLCS problem and evaluated their performance.  The first combination
is a {\em sequential} implementation of Peng's algorithm using
disjoint set on {\em both} the first stage and second stage.  The
other three combinations are implemented in parallel environment.  The
{\em parallel-ST-disjoint} combination uses disjoint set in the first
stage and sparse table in the second stage.  The {\em
  parallel-COST-disjoint} combination uses disjoint set in the first
stage and compressed sparse table in the second stage.  Finally the
{\em parallel-COST} combination uses compressed sparse table in both
first and second stage.

We compare the performance of the four combinations on input strings
generated {\em randomly} from the alphabet $\{A, T, C, G\}$.
Figure~\ref{fig:fig-parallel} shows the execution time of all
combinations under different lengths of input.  We note that the
compressed sparse table implementation outperforms the theoretically
better $O(1)$ sparse table implementation.  That is, {\em   parallel-
COST} outperforms all other parallel implementations.

% Why is compressed sparse table the best?

There are two main reasons to lead the theoretically $O(1)$ sparse table
slow. The first reason is that the encoding Cartesian tree needs more
instruction cycles.  The second reason is that the block size is limited
by the theorem, which would bright more cache miss with large lookup
table.

Figure~\ref{fig:fig-parallel-scala} shows the scalability of the best
parallel combination {\em parallel-COST}.  It achieves a speedup of
$8$ than the serial implementation on our server with 6 cores and
hyper-threading.

\begin{figure}
  \centering
  \subfigure[Runtime]{
    \includegraphics[width=0.45\linewidth]{\GraphicPath/fig-parallel-n.pdf}
    \label{fig:fig-parallel}
  }
  \subfigure[Scalability]{
    \includegraphics[width=0.45\linewidth]{\GraphicPath/fig-parallel-p.pdf}
    \label{fig:fig-parallel-scala}
  }
  \caption{The execution time and scalability results of our parallel
    implementations on an E5-2620 server with 6 cores and
    hyper-threading}
\end{figure}


\subsection{Incremental Suffix Maximum Query}

We now compare the performance of {\em four} data structures for
supporting incremental suffix maximum query only.  The experiments
alternate between appending a data and issuing a range query.  Both
the length and the position of the range query are uniformly
distributed among all possibilities.

The first data structure is {\em disjoint set}.  We implemented path
compression and rank strategies so that the amortized time
$o(\alpha(n))$.  The second data structure is {\em sparse table}.  The
insertion time is $O(\log n)$, and the query time is $O(1)$.  The
sparse table is allocated in row-major to reduce cache miss.  Please
refer to Algorithm~\ref{alg:parallel-VGLCS} and
Figure~\ref{fig:interval-decomposition} for details.  The third data
structure is {\em compressed sparse table}.  The amortized insertion
time is $O(1)$ and the query time is $O(s)$, where $s$ is the size of
the block and is set to $16$ in the experiments.  Please refer to
Section~\ref{sec:parallelRMQ} for details.  The final data stricture
is {\em amortized sparse table}.  Both the amortized insertion time
and the query time are $O(1)$.  In our implementation we set the block
size $s$ to $8$, and apply other similar optimization as in compressed
sparse table.

Figure~\ref{fig:fig-ISMQcmp} indicates that our compressed sparse
table implementation runs faster than the disjoint set implementation
when the number of data $n$ is greater than $10^6$.  In particular
when $n$ is greater than $10^7$, the compressed sparse table runs
$1.8$ times faster than the disjoint set.

\begin{figure}[!thb]
  \centering
  \includegraphics[width=0.45\linewidth]{\GraphicPath/fig-ISMQ.pdf}
  \caption{The performance of the different data structures for
    supporting incremental suffix maximum query on an E5-2620 server}
  \label{fig:fig-ISMQcmp}
\end{figure}

% I have no idea what the following sentences try to say.

We conduct other experiment about more complex scenario.  That is, in
the dynamic programming, the tendency of insertion value will inflict
the performance of data structure, so we consider the different
probability for each data structure.

The number of the query is ten times of the number of insertions.  There
are two strategies for each $(p, q, L)$, implemented by compressed
sparse table and theoretically better $O(1)$ sparse table.  The result
is shown on table~\ref{tlb:ISMQcmp}.

Table~\ref{tlb:ISMQcmp} show the performance of the ISMQ when we set
the number of data $N$ to $10^7$, and we vary the maximum interval
sizes $L$, the probability $p$ of having an larger next element, and
the probability $q$ of an element being zero.

The amortized $O(1)$ sparse table run $1.5 \times$ faster than the
compressed table in above situation.

\input{\TablePath/tlb-ISMQcmp.tex}

\subsection{Parallel Range Maximum Query}

Each stage will have $n$ number of elements and the $n$ number of the
query.  In this special cases, we run the compressed sparse table to
improve the performance compared to the origin.  If we preprocess all
range size and mark the necessary compressed blocks before building, we
get the optimized version which run $2.35$ faster than origin sparse
table. The Table~\ref{tlb:CORMQ} is shown the result of the strategy of
the parallel range query.

\input{\TablePath/tlb-CORMQ.tex}
