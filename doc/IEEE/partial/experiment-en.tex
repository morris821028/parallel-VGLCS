\section{Experiment}
\label{sec:Experiment}

For the experiments we used a Server with Intel Xeon E5-2620 2.4 Ghz
processor and 384 KiB of level 1 cache, 1536 KiB of level 2 cache, 15
MiB of shared level 3 cache, and running Ubuntu 14.04.  The Intel CPU
supports hyper-threading technique, and each processor has six cores.
All algorithms were implemented in C++ and OpenMP compiled with GCC
using the \texttt{-O2} and \texttt{-fopenmp} flag.

\iffalse
我們運行在 Intel Xeon E5-2620 2.40 GHz 主機上，其擁有 L1 cache 384 KiB、L2 cache 1536 KiB 和 L3 cache 15 MiB，
Intel CPU 同時也支持 hyper-threading 技術，每個處理器有 6 個實體核心。
所有的演算法使用 C++ 和 OpenMP 實作，使用優化參數為 \texttt{-O2} 和 \textt{-fopenmp}。
\fi

\subsection{VGLCS with Compressed Sparse Table}

In the random test data, we get the better performance with compressed
sparse table compared to theoretical $\theta(1)$ sparse table.  The
figure \ref{fig:fig-parallel} shows the different runtime in different
parallelism, and the figure \ref{fig:fig-paralell} shows the
scalability of our parallel algorithm.  On our server, the parallel
algorithm can get $8 \times$ faster than the serial algorithm.

\begin{figure}[!thb]
  \centering
  \subfigure[Runtime]{
    \includegraphics[width=\linewidth]{graphics/fig-parallel-n.pdf}
    \label{fig:fig-parallel}
  }
  \subfigure[Scalability]{
    \includegraphics[width=\linewidth]{graphics/fig-parallel-p.pdf}
    \label{fig:fig-parallel-scala}
  }
  \caption{Serial and Parallel Algorithm run on E5-2620}
\end{figure}

%\subsection{理論常數 VGLCS}

%尚未完成

\subsection{Incremental Suffix Maximum Query}

In VGLCS problem, the number of insertion equals the number of the
queries. We compare the performance between the four data structure as
follows:

\begin{itemize}
  \item 

Disjoint Set: Average run time $o(\alpha(n))$.  It is implemented by
path compression.

  \item 

Sparse Table: Insertion in $O(\log n)$ time, and query in $O(1)$ time.
We allocate $\tt{table}[\log N][N]$ which arranged in row-major to
reduce cache miss.

  \item 

Binary Indexed Tree: Both insert and query operation consume $O(\log
n)$ time.

  \item 

Compressed Sparse Tree: The insertion operation consume amortized
$O(1)$ time.  The query operation consumes $O(s)$ time, which $s$ is
the size of the block.  We can maintain extra the prefix and suffix
maximum in block to reduce the time complexity $O(s)$ to $O(1)$ in
most queries.

\end{itemize}

The figure \ref{fig:fig-ISMQcmp} shows that our compressed sparse
table run faster than the disjoint set when $n > 10^6$.  When $n >
10^7$, the compressed sparse table run faster $1.25 \times$ than the
disjoint set.  

\begin{figure}[!thb]
  \centering
  \includegraphics[width=0.9\linewidth]{graphics/fig-ISMQ.pdf}
  \caption{ISMQ runs on E5-2620 with different data structures. We use the random test cases without any limitation, e.g. the length of interval query has a uniform distribution.}
  \label{fig:fig-ISMQcmp}
\end{figure}

However, we observe that our amortized $\theta(1)$ sparse table is
more slow than disjoint set.  In the dynamic programming, the tendency
of insertion value will inflict the performance of data structure, so
we experiment the different probability for each data structure.  The
number of the query is 10 multiple of the number of insertion.  The
result is shown on table \ref{tlb:ISMQcmp}.  When $N=10^7$, our
amortized $\theta(1)$ sparse table run faster $1.26 \times$ than the
disjoint set.

\input{./tables/tlb-ISMQcmp.tex}

\subsection{Parallel Range Query}

Each stage will have $n$ number of elements and the $n$ number of the
query.  In this special cases, we run the \texttt{CORMQ} (compressed
RMQ) to improve the performance compared to the origin.  If we can
predict all range size before building, we get the \texttt{CORMQ-opt}
which run $2.35$ faster than origin RMQ.  The table \ref{tlb:CORMQ} is
shown the result of the strategy of the parallel range query.

\input{./tables/tlb-CORMQ.tex}