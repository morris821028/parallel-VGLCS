\section{Experiment} \label{sec:Experiment}

We conduct experiments on an Intel Xeon E5-2620 2.4 Ghz processor with
384K bytes of level 1 cache, 1536K bytes of level 2 cache, and 15M
bytes of shared level 3 cache.  The Intel CPU supports
hyper-threading, and each processor has six cores.  The operating
system is Ubuntu 14.04.  We implemented all algorithms in C++ and
OpenMP and compiled them using gcc with {\tt -O2} and {\tt -fopenmp}
flag.

We conduct three sets of experiments.  The first set evaluates various
data structures for supporting different queries of VGLCS problem.
The second set purely evaluates the performance of incremental suffix
maximum query using various data structures.  The third set evaluates
the performance of parallel range query.

\subsection{Variable Gapped Longest Common Subsequence}

We implement {\em disjoint set} and {\em sparse table} in our
experiments and evaluate their performance on VGLCS.  For disjoint set
we implemented path compression and rank strategies so that the
amortized time $o(\alpha(n))$.  Its variant, {\em incremental tree set
  union}~\cite{Gabow1983ALA}, has an amortized time $O(1)$ for each
query.  The sparse table has insertion time $O(\log n)$, and query
time $O(1)$.  The table is allocated in row-major to reduce cache
miss.  Please refer to Algorithm~\ref{alg:parallel-VGLCS} and
Figure~\ref{fig:interval-decomposition} for details.

We implemented four combinations of data structures in solving the VGLCS
problem and evaluated their performance.  The first combination is a
{\em sequential} implementation of Peng's algorithm using disjoint set
on {\em both} the first stage and second stage.  The other three
combinations are implemented in parallel environment.  The {\sc
Para-ST-DS} combination uses disjoint set in the first stage and sparse
table in the second stage.  The {\sc Para-CoST-DS} combination uses
disjoint set in the first stage and compressed sparse table in the
second stage. Finally the {\sc Para-CoST} combination uses compressed
sparse table in both first and second stage.

We compare the performance of the four combinations on input strings
generated {\em randomly} from the alphabet $\{A, T, C, G\}$.
Figure~\ref{fig:fig-parallel} shows the execution time of all
combinations under different lengths of input.  We note that the
compressed sparse table implementation outperforms the theoretically
better amortized $O(1)$ sparse table implementation.  That is, {\em
parallel-COST} outperforms all other parallel implementations.  We
believe that there are two reasons for the theoretically better $O(1)$
sparse table implementation actually runs slower.  First, the encoding
Cartesian tree needs more instruction cycles to complete.  Second,
despite that in theory~\cite{Fischer2006TheoreticalAP} the optimal block
size is ${\log n}\over{4}$, this block size is usually too large for
implementations. That is, we will need to build a very large LCA lookup
table, but we will {\em not} access all of it, which means it is very
unlikely that we will access contiguous memory, which causes serious
cache misses.

Figure~\ref{fig:fig-parallel-scala} shows the scalability of the best
parallel combination {\em parallel-COST}.  It is {\em eight} times
faster than a serial implementation on our server with 6 cores and
hyper-threading.

\begin{figure}
  \centering
  \subfigure[Runtime]{
    \includegraphics[width=0.45\linewidth]{\GraphicPath/fig-parallel-n.pdf}
    \label{fig:fig-parallel}
  }
  \subfigure[Scalability]{
    \includegraphics[width=0.45\linewidth]{\GraphicPath/fig-parallel-p.pdf}
    \label{fig:fig-parallel-scala}
  }
  \caption{The execution time and scalability results of our parallel
    implementations on an E5-2620 server with 6 cores and
    hyper-threading}
\end{figure}


\subsection{Incremental Suffix Maximum Query}

We now compare the performance of {\em four} data structures for
supporting incremental suffix maximum query only.  In addition to
disjoint set and sparse table described earlier, we also implemented
{\em compressed sparse table} and {\em amortized $O(1)$ sparse table}.
The compressed sparse table has an amortized insertion time $O(1)$ and
query time $O(s)$, where $s$ is the size of the block and is set to $16$
in the experiments.  Please refer to Section~\ref{sec:parallelRMQ} for
details.  The amortized sparse table has $O(1)$ for both amortized
insertion time and the query time.  In our implementation we set the
block size $s$ to $8$, and apply other similar optimization as in
compressed sparse table.

We first tried a simple scenario in which we {\em alternate} between
appending a data and issuing a range query.  Both the length and the
position of the range query are uniformly distributed among all
possibilities.  Figure~\ref{fig:fig-ISMQcmp} shows the performance of
the four data structures for supporting incremental suffix maximum
queries under this simple scenario.  We note that our compressed sparse
table implementation runs faster than the disjoint set implementation
when the number of data $n$ reaches $10^6$.  In particular when $n$ is
greater than $10^7$, the compressed sparse table runs $1.8$ times faster
than the disjoint set.

\begin{figure}[!thb]
  \centering
  \includegraphics[width=0.45\linewidth]{\GraphicPath/fig-ISMQ.pdf}
  \caption{The performance of the different data structures for
    supporting incremental suffix maximum query on an E5-2620 server}
  \label{fig:fig-ISMQcmp}
\end{figure}

Now we conduct experiments in a more complex scenario.  In a dynamic
programming various factors affect performance.  These factors include
the distribution of values being inserted, the maximum interval being
queried, and the ratio between the numbers of insertion and queries.
For ease of notation we use $p$ to denote the probability of having an
larger next element to insert, $q$ for the probability of inserting a
zero, and $L$ for the maximum interval being queried.  In our
experiments we set the number of data $N$ to be $10^7$, and vary the
maximum interval sizes $L$ from 4 to 16, and vary $p$ and $q$ from 0
to 100\%.  We also set the number of the queries to be ten times of
the number of insertions.

Table~\ref{tlb:ISMQcmp} compares timing of answering incremental
suffix maximum query using compressed sparse table and the
theoretically better amortized $O(1)$ sparse table.  We observe that
the sparse table runs up to $1.5$ times faster than the compressed
table.

\input{\TablePath/tlb-ISMQcmp-new.tex}

We also observe that the probability $p$ affects the performance gain
of our ``peeking'' technique.  When the probability $p$ is close to 0
or 1 the peeking achieves excellent performance gain.  The performance
gain of the peeking operation also depends the block size.  In
addition, the block size best for LCA sparse table is different from
the best block size for bit-compressed sparse table.
We also observe that the probability $p$ affects the performance gain of
our ``peeking'' technique.  When the probability $p$ is close to 0 or 1,
the peeking achieves excellent performance gain.  The performance gain
of the peeking operation also depends the block size.  In addition, the
block size best for LCA sparse table is different from the best block
size for bit-compressed sparse table.

We also observe that the performance of the compressed sparse table is
better than that of $O(1)$ sparse table, but the performance advantage
disappears when $q$ is close to 1. $O(1)$ sparse table requires the
Cartesian index computation, which the compressed sparse table does not.
When the probability $q$ is close to 1, $O(1)$ sparse table will require
less instructions on computing Cartesian index, so its performance will
be close to that of the the compressed sparse table.

The maximum length of interval query ($L$ in Table~\ref{tlb:ISMQcmp})
affects the performance of interval query in a compressed sparse table,
but not of $O(1)$ sparse table.  Note that in
Algorithm~\ref{alg:cartesian64bits-query} the block size $s$ is set to
16, which is the maximum number of times the loop in
Algorithm~\ref{alg:cartesian64bits-query} will iterate for an query.
Therefore the query time will increase when $L$ increases.  On the other
hand, the $O(1)$ sparse table answer a query by a single lookup into the
LCA table, so its performance is not affected by $L$.

%% The computation of Cartesian index maybe can be implemented well.
%% In order to hide the optimized detail in implementation, we conduct
%% this experiment.

\subsection{Parallel Range Maximum Query}

We now compare the performance of {\em two} data structures for
supporting parallel range maximum query only.  Since the disjoint set
only supports suffix query so we will not consider it here.  Instead we
will consider {\em sparse table} ({\sc ST}) and {\em compressed sparse
table} ({\sc CoST}). The implementation computes all query range sizes,
and mark only the necessary compressed blocks to build before querying.

Table~\ref{tlb:CORMQ} compares the timing results of the three data
structures for supporting parallel range query.  We set the number of
elements $N$ to be the same as the number of range queries.  We observe
that the optimized compressed sparse table is $1.4$ times faster than
sparse table when $N$ reaches $100000$.

% Make more observation on $L$ and $N$.

The number of input elements ($N$ in Table~\ref{tlb:CORMQ}) verify the
scalability of Algorithm~\ref{alg:parallel-sparse-table}.  The maximum
length of interval query ($L$ in Table~\ref{tlb:CORMQ}) affects the
performance of the access different level in sparse table. The number of
level in the compressed sparse table is small than in the native sparse
table.  Therefore, when $L$ is small, there is a high probability that
the interval query can be processed in the same level, and also affects
performance of cache temporal locality for compressed sparse table
because multiple data store in compressed block. Oppositely, when $L$ is
large, data sharing decreases the efficiency during integrating the
maximum value by the definition of recursion.

\input{\TablePath/tlb-CORMQ.tex}
