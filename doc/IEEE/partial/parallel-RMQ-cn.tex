\section{Parallel VGLCS Algorithm} \label{sec:parallelVGLCS}

\subsection{Basic Dynamic Programming}

We first describe a basic dynamic programming for
VGLCS~\cite{Peng2011TheLC}. Let $A$ and $B$ denote two input strings
of length $n$ and $m$ respectively, and $G_A$ and $G_B$ be the arrays
of the variable gap constraints.  We define $V[i][j]$ to be the {\em
  maximum} length of the variable gapped longest common subsequence
between substring $A[1, i]$ and $B[1, j]$.  It is easy to see that
$V[i][j]$ is the {\em maximum} among $V[k][l]$, where $k$ is between
$i-1$ and $i-G_A(i)-1$, and $l$ is between $j-1$ and $j-G_B(j)-1$,
i.e., a rectangle within $V$ on the left and upper of $V[i][j]$.
Please refer to Figure~\ref{fig:fig-VGLCS-dp-naive} for an
illustration.

\begin{figure}[!thb]
  \centering \subfigure[How to compute $V$.] {
    \includegraphics[width=0.45\linewidth]{\GraphicPath/fig-VGLCS-dp-naive.pdf}
    % \caption{How to compute $V$.}
    \label{fig:fig-VGLCS-dp-naive}
  } \subfigure[Compute $V$ with incremental suffix maximum queries.] {
    \includegraphics[width=0.45\linewidth]{\GraphicPath/fig-VGLCS-dp.pdf}
    \label{fig:fig-VGLCS-dp}
  }
  \caption{The basic dynamic programming for VGLCS}
  \label{fig:basic-dp-VGLCS}
\end{figure}

The computation of $V$ can be optimized as follows.  Note that the
computation of all $V[i][j]$'s with the same $i$ has the {\em same} gap
constraint $G_A(i)$, so the maximum within the rectangle can be computed
in two steps.  First, we compute the maximum of {\em every column} of
this rectangle, and place them into another array $R$. Then we compute
the maximum of the {\em suffix} of length $G_B$ on $R$, which is exactly
$V[i][j]$.  Please refer to Figure~\ref{fig:fig-VGLCS-dp} for an
illustration.

We note that this optimization requires maximum queries on the suffix
{\em incrementally} in the following sense.  Recall that in the first
step of the optimization, we need to compute the maximum of every column
within the rectangle.  This is just like finding the maximum of the {\em
suffix} of every column in that rectangle.  After we compute the $i$-th
row of $V$ and go to the next row to compute $V[i+1][*]$, we will then
need the maximum of the suffix of every column of length $G_A(i+1)$. It
will be beneficial if we put the $V$'s in each column into a data
structure that supports suffix maximum query.  Similarly, the
computation within the same row, that is, from $V[i][j]$ to $V[i][j+1]$,
also requires the maximum of the suffix on $R$.  We will refer to this
type of queries as {\em incremental suffix maximum query}, i.e., we
would like to maintain a data structure form which we can find the
maximum of its suffix efficiently while we add data at its end.

% \begin{figure}[!thb]
%   \includegraphics[width=0.40\linewidth]{\GraphicPath/fig-VGLCS-dp.pdf}
%   \caption{Compute $V$ with incremental suffix maximum queries.}
%   \label{fig:fig-VGLCS-dp}
% \end{figure}

\subsection{Peng's Algorithm}

The sequential VGLCS algorithm~\ref{alg:serial-VGLCS} by
Peng~\cite{Peng2011TheLC} applies the optimization and is shown as
Algorithm~\ref{alg:serial-VGLCS}.  The outer loop goes through every
row, and the inner loop goes through every element of a row from left to
right.  We use an array of $C$ to answer incremental maximum queries on
all columns.  That is, we can think of $C[j]$ as a data structure that
supports incremental suffix maximum queries on the $j$-th column of $V$.
From the previous observation that all computation of elements in the
$i$-th row of $V$ share the same gap $G_A(i)$, we will query each $C$
for the maximum in the suffix that ends at row $i-1$ with length
$G_A(i)+1$, and place these maximums in another data structure $R$ that
also supports incremental suffix maximum queries.  It is easy to see
that the value of $V[i][j]$ can be obtained by querying $R$ with a
suffix maximum query of length $G_B(j)+1$ as shown in
Figure~\ref{fig:fig-VGLCS-dp}.

We update $V$, $C$, and $R$ as follows.  If the $i$-th character of $A$
matches the $j$-th character of $B$ then $V[i][j]$ is the maximum among
the rectangle plus 1, as shown in Figure~\ref{fig:fig-VGLCS-dp}. This
maximum can be found by querying $R$ for the maximum among the last
$G_B(j)$ elements in it.  Note that $R$ contains the information of the
previous row, up to the element of the $j-1$ element.  After that, we
add the maximum of last $G_B(j)+1$ in the $j$-the column into $R$, and
the newly computed $V[i][j]$ into $C[j]$, which supports ISMQ on the
$j$-the column, before going to column $j+1$.  If the $i$-th character
of $A$ does {\em not} match the $j$-th character of $B$, we simply set
$V$ to 0 since it does not affect the answer, then again update $R$
accordingly.
 
 
\input{\AlgoPath/alg-serial-VGLCS-2e}

\subsection{Incremental Suffix Maximum Query}

From the previous discussion of Peng's algorithm, we note that in order
to find VGLCS efficiently, we need to address the {\em incremental
suffix maximum query} (ISMQ) problem.  A data structure that supports
incremental suffix maximum queries should support the three operations.
First, a {\sc Make} operation creates an empty array $A$. Second, an
{\sc Append}$(V)$ operation appends a value $V$ to array $A$. Finally, an
ISMQ {\sc Query}$(x)$ finds the {\em maximum} value among those from $x$
to the end of an array $A$.

Peng uses a {\em disjoint-set} data structure to answer incremental
suffix maximum queries in his VGLCS algorithm.  The disjoint-set data
structure was proposed by Gabow~\cite{Gabow1983ALA} and
Tarjan~\cite{Tarjan1975EfficiencyOA} to solves the {\em union-and-find
problem}.  The set of data is stored in a sequence of disjoint sets, and
the {\em maximum} of each disjoint set is at the root of the tree, and
these maximum are in {\em decreasing} order.  When we add a value $x$,
we make it as a disjoint set with a single element by itself, and as the
{\em last} disjoint set in the sequence.  Then we start joining (with
union operation) from the last set to its previous set until the maximum
of the previous set is {\em larger} than $x$. It is easy to see that the
{\sc Query(x)} operation is simply a {\em find} operation that finds the
root, which has the {\em maximum}, of the tree that $x$ belongs to.  The
amortized time per union/find operation is $O(\alpha(n))$.

\subsection{A Parallel VGLCS Algorithm with Sparse Table}

The sequential VGLCS algorithm~\ref{alg:serial-VGLCS} by
Peng~\cite{Peng2011TheLC} and other variants of LCS are difficult to
parallelize in a row-by-row manner.  These algorithms use several
states to determine a new state with a dynamic programming.  This
construction requires {\em heavy data dependency}, and is difficult to
parallelize in a naive row-by-row manner because an element of the
dynamic table needs the values of elements in the {\em same} row to
compute its value.

It is also difficult to parallelize Peng's algorithm with the wavefront
method because it requires {\em extra space} to keep track of row
status, which is not required in a sequential algorithm. Recall that
VGLCS requires a rectangle of data in $V$ to compute a new element, if
those $V$'s want to compute on a diagonal wavefront, those rectangles of
data will require extra bookkeeping since the gap constraints of those
elements on the wavefront could be very different.  Please refer to
Figure~\ref{fig:fig-VGLCS-dp-wavefront} for an illustration of those
data that must be present (in solid line).  Also, the new elements
computed must be appended to the data strictures according to the
wavefront, which incurs more bookkeeping and overheads.

\begin{figure}[!thb]
  \centering \subfigure[The first stage]{
    \includegraphics[width=0.45\linewidth]{\GraphicPath/fig-VGLCS-dp-wavefront-second.pdf}
  } \subfigure[The second stage]{
    \includegraphics[width=0.45\linewidth]{\GraphicPath/fig-VGLCS-dp-wavefront-first.pdf}
  }
  \caption{The bookkeeping data of the wavefront method}
  \label{fig:fig-VGLCS-dp-wavefront}
\end{figure}

We propose a parallel VGLCS algorithm with an optimized row-by-row
approach, which maintains only {\em one} row data structure that
collects suffix maximum values from all columns.  That is, our
optimized row-by-row approach removes the data dependency among
elements {\em within the same row}.

Our optimized row-by-row approach uses less space, has a more balanced
workload, and a smaller thread synchronization overhead, than the
wavefront method.  We observe that the length of the critical path of
a wavefront method is greater than that of a row-by-row method.  In
addition, if we can removes the data dependency among elements within
the same row, then the computation on the elements of the same row can
be {\em fully} and {\em evenly} parallelized.  The result is a much
more balanced workload distribution and a much easier synchronization
among threads.


% On the other hand, if we use the
% Maleki's~\cite{Maleki2016EfficientPU} technique, % need to explain
% this technique it also uses extra space to maintain state
% translation, and spends more time to merge data.  Therefore, it is
% crucial that our parallelization conserves {\em both} memory and
% time.

% It seems that you should use the new figures to explain these???
% Morris: how to make it to row-by-row manner


A sketch of our algorithm is as follows.  Our algorithm computes $V$ one
row at a time.  The computation of each row has two stages.  In the
first stage, the algorithm queries each data structure $C$ for every
column within the rectangle {\em in parallel}, so as to obtain the
maximums of suffix of length $G_A(i) + 1$ of every column, and place
them into an array $R$. Recall from Algorithm~\ref{alg:serial-VGLCS}
that every column of $V$ has a data structure $C$ that supports
incremental suffix maximum on $V$.  Please refer to
Figure~\ref{fig:fig-VGLCS-dp-rmq} for an illustration.

In the second stage, our algorithm issues {\em range maximum queries},
one for each column, on $R$ to compute all $m$ elements of the $i$-th
row of $V$ {\em in parallel}.  Note that unlike the sequential
algorithm, we compute all elements in the $i$-th row of $V$ in parallel,
so we cannot query the {\em suffix} of $R$.  Instead, we need to query a
{\em range} of $R$ for the maximum, where the range is the gap
constraint on that column.  Please refer to
Figure~\ref{fig:fig-VGLCS-dp-rmq} for an illustration.  Note that we
need to add the newly computed $V[i][j]$ into the $C$ of the $j$-th
column {\em incrementally}, so that they will contain the correct
information for the computation of the $(i+1)$-th row.  Also, since the
algorithm iterates in rows, these $C$'s only need to support suffix
maximum query.  No range query on them is required.  In contrast, we do
need to support range maximum query on $R$, and these queries will be in
parallel.

\begin{figure}[!thb]
  \centering \subfigure[The first stage]{
    \includegraphics[width=0.45\linewidth]{\GraphicPath/fig-VGLCS-dp-rmq-first.pdf}
  } \subfigure[The second stage]{
    \includegraphics[width=0.45\linewidth]{\GraphicPath/fig-VGLCS-dp-rmq-second.pdf}
  }
  \caption{Two stages of the computation of one row of $V$.}
  \label{fig:fig-VGLCS-dp-rmq}
\end{figure}

To resolve the data dependency, we need to consider a good data
structure that can handle incremental suffix/range maximum query {\em in
parallel}.  We note that it is {\em not} feasible to parallelize the
disjoint set implementation for three reasons.  First, a query for
disjoint set will change the data structure because a lookup will {\em
compress} the path to the root.  It is difficult to maintain a
consistent view of the data structure when multiple threads are
compressing the path {\em simultaneously}.  Second, when multiple
threads are compressing different paths, the load among them could be
very different, and this will incur load imbalance.  Third, there will
be a large number of threads working on different parts of the disjoint
set, therefore it will be difficult to synchronize them efficiently.

\subsubsection{Sparse Table} \label{sec:sparse-table}

Since the disjoint set cannot be implemented efficiently in parallel,
we use {\em sparse table}~\cite{Berkman1993RecursiveSP} to support
incremental suffix/range maximum queries in our VGLCS algorithm.
Sparse table~\cite{Berkman1993RecursiveSP} requires a $O(n \log n)$
preprocessing, and can support range maximum query in $O(1)$ time on
one dimensional data.  A sparse table is a two dimensional array.  The
element of a sparse table in the $j$-th row and $i$-th column is the
maximum among the $i$-th elements and its $2^j - 1$ predecessors in
the input array.

We give an example of the sparse table
(Figure~\ref{fig:interval-decomposition}).  The input is in array $A$.
We split array $A$ into five blocks so that each block has four
elements.  Then we build a sparse table $T$ on $A$ as described
earlier.  Now a ranged maximum query on $A$ can be answered by at most
{\em two} queries into the sparse table.  For example, if the query is
from 2 to 13, then the answer is the maximum of from 2 to 9
($T[3][9]$), and from 6 to 13 ($T[3][13]$).  Both are from the third
level of the table since each has the maximum of $2^3 = 8$ elements in
the input.

\begin{figure}[!thb]
  \centering \subfigure[Array]{
    \includegraphics[width=0.45\linewidth]{\GraphicPath/fig-interval-decomposition-origin.pdf}
    \label{fig:fig-interval-decomposition}
  } \subfigure[Sparse table]{
    \includegraphics[width=0.45\linewidth]{\GraphicPath/fig-sparse-table-origin.pdf}
    \label{fig:fig-sparse-table}
  }
  \caption{A sparse table example}
  \label{fig:interval-decomposition}
\end{figure}

It is easy to see that one can build a sparse table in parallel
efficiently.  Please refer to
Algorithm~\ref{alg:parallel-sparse-table} for details.
Algorithm~\ref{alg:parallel-sparse-table} builds sparse table in
parallel and in $O(n \log n / p + \log n)$ time, where $n$ is the
number of elements and $p$ is the number of processors.  This
algorithm is very easy to parallelize and implement.

\input{\AlgoPath/alg-parallel-sparse-table-2e}

\subsubsection{A Parallel VGLCS with Sparse Table}

The operations on a sparse table are much easily to parallelize than
those on a disjoint set, which is used within the inner loop of Peng's
sequential VGLCS algorithm.  The inner loop of Peng's algorithm
alternates between append and query operations on $R$.  Please refer
to Algorithm~\ref{alg:serial-VGLCS} for details.  This alternation
between appending and querying incurs heavy data dependency.  In
addition, the parallelism of operations on a disjoint tree is limited
by the length of path under compression.  The length is usually very
short and provides very limited parallelism.

The pseudo code of our parallel VGLCS algorithm with sparse table is
given in Algorithm~\ref{alg:parallel-VGLCS}.  The algorithm computes
$V$ one row at a time.  The computation of each row has two stages.
In the first stage, the algorithm queries the $C$'s {\em in parallel}
to obtain the maximums of suffix of length $G_A+1$ and place them
into an array $R$.  Then we build a sparse table $T$ with the data of
$R$.  Then in the second stage, the algorithm queries $T$ to find the
range maximum in $R$ to compute all elements in the $i$-th row of $V$
{\em in parallel}.

\input{\AlgoPath/alg-parallel-VGLCS-2e}

The implementation of the two stages have different challenges.  The
first stage is easier to parallelize because the operations on
individual columns are {\em independent}.  However, it will insert new
data into $C$, and still needs to answer suffix queries efficiently in
order to build the $R$ array.  The second stage does {\em not}
requires insertion so it is more static.  However, since we compute
all $V$'s in the same row in parallel, it requires {\em ranged
  queries}, instead of suffix queries, on the sparse table $T$.  The
next two sections will describe our approaches to address these
challenges of two stages.  For ease of presentation we will describe
our approach for the second stage in Section~\ref{sec:parallelRMQ}
first.  Then we address the first stage in Section~\ref{sec:QIUD}.
