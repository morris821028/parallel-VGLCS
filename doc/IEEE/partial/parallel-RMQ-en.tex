\section{Ranged Maximum Query} \label{sec:parallelRMQ}

% Why Ranged Maximum Query is important?

The efficiency of a parallel VGLCS algorithm is limited by the the
efficiency of its {\em ranged maximum query}.  We can use a parallel
sparse table implementation to answer ranged maximum queries with $p$
processors, so that it requires $O(n \log n / p + \log n)$ in
preprocessing, and takes only $O(1)$ time to answer a query.  On the
other hand, it is difficult to parallelize both preprocessing and
query on tree-like data structures efficiently.

\subsection{Blocked Sparse Table}

Fischer~\cite{Fischer2006TheoreticalAP} consider a {\em blocked}
version of the sparse table.  The input is partitioned into $s$ blocks.
We now have two types of queries -- {\em super block} query and a {\em
  in-block} query.  A super block query queries the answer for
consecutive blocks, and a in-block query queries a segment {\em
  within} a block.  Then we compute the maximum for each blocks, and
compute a sparse table $T_s$ for these maximums.  It is easy to see
that we can query this sparse table $T_s$ {\em twice} to answer any
super block query.  An in-block queries can be answered by a single
lookup into an {\em index table} that encodes the elements in block
into a {\em Cartesian tree index}.  More details will be provided
later.  Since we can split {\em any} ranged query into at most a
super-block queries and two in-block queries, we need at most four
memory access to answer any ranged query.

Fischer's algorithm~\cite{Fischer2006TheoreticalAP} computes an {\em
  index table} for every block.  The algorithm scans through the data
within a block, and places the data within a maximum {\em Cartesian
  tree}.  One can think of this Cartesian tree as a heap where the
data are in heap order and the indexes of the data are in sorted
binary tree order.  As a result any ranged query from the $i$-th to
the $j$-th element is located at the least common ancestor in heap.
Let the index of this least common ancestor be $k$. then we add the
key value pairs $((i, j), k$ into the index table of this block.  Note
that the algorithm does {\em not} maintain the value the $k$-th
element.  Instead it keeps the {\em index} of the least common
ancestor so that two blocks with the same relative key order can share
the the same index table.  For example, the blocks $(10, 20, 30)$ and
$(101, 202, 303)$ can share the same index table in which $(1, 2)$ is
$2$.  That is the ranged maximum query from the first to the second
element will return the {\em second} element.  It is easy to see that
there are at most $C_s$ different index tables, where $C_s$ is $s$-th
Catalan number, i.e. the number of different Cartesian trees of $s$
nodes.  Also $C_s = \frac{1}{s+1}\binom{2s}{s} =
O(\frac{4^s}{s^{1.5}})$.  By knowing which Catalan index a block
belongs to, the algorithm can use the index table, which can be
identified by its Catalan index, and look into the correct index
table.

Fischer's algorithm~\cite{Fischer2006TheoreticalAP} builds index
tables by choosing $s = \frac{1}{4} \log n$ as the block size.  As a
result and preprocessing time is $O(n)$, and the space complexity is
$O(s^2 \frac{4^s}{s^{1.5}}) = O(n)$.  As a result a sequential version
requires $O(n)$ time in preprocessing, and $O(1)$ time in answering a
query, and both preprocessing time and query answering are the best.

Fischer's algorithm causes {\em serious cache miss} When the number of
data is large.  Fischer's algorithm will construct index tables for
blocks in a on-demand manner.  When the algorithm finds that the
corresponding index table is {\em not} present in memory, it will
build it in memory, which will be cached.  This process will repeat
the number of blocks times, and causes cache misses.

In order to reduce cache miss, Demaine~\cite{Demaine2009OnCT} proposed
{\em cache-aware} operations on Cartesian
tree~\cite{Vuillemin1980AUL}.  Therefore Demaine's algorithm skip the
checking for existing index table, and builds a index table for {\em
  every} block.  In addition, Demaine's algorithm uses a binary string
of length $2s$ as the identifier of a block using Cartesian tree.  One
can answer ranged maximum queries directly from examining this binary
string.  However, this operation requires counting the number of 1's
between the last two 0's, therefore it is {\em hard} to implement
efficiently in a modern computer.

% We replace lookup operation to naive operation.  The naive operation
% is find the maximum value by comparing each element on the compressed
% data. On the other hand, the lookup operation find the index of
% maximum value from the index table. When loading a element from index
% table, it also bring some useless data to caches.   In order to use
% cache efficiently, the naive operation is better than the lookup
% operation because the access pattern is almost one by one in our VGLCS
% algorithm.

\subsection{Compressed Cartesian Tree} \label{sec:cct}

We propose a new encoding for blocks called {\em compressed sparse
  table}, instead of the binary string, in order to improve
performance.  The compressed sparse table is inspired by Fischer's
sparse table and the Cartesian tree.  We also use the block approach
and select a fixed block size $s = {{\frac{\log n}{4}}} = 16$, which
can efficiently answer one-dimension ranged maximum query for up to $n
= 2^{64}$ data.

The compressed sparse table works by keeping only the {\rm rightmost}
path of the Cartesian tree.  Consider the processing of adding a data.
Let the $i$-th added data be $a_i$, so we need to {\em relocate} all
the data along the rightmost path of the Cartesian tree that are
smaller than $a_i$.  Let $t_i$ be the number of nodes that need to be
relocated, and it is easy to see that $0 \le t_i < s$.

The key point of compressed sparse table is that we store $t_i$ to
{\em implicitly identify} this Cartesian tree for this block of data,
so that we can answer queries by examining these $t_i$'s.  More
details on how to answer queries with these $t_i$ will be described
later.  Now since all $t_i$'s are small than 16, we can represent each
of them as a 4-bit integer.  We then merge 16 of these 4-bit integers
into a 64-bit integer to present a Cartesian tree for a block.  The
pseudo code on how to build a 64 bit integer to represent a block of
16 data is in Algorithm~\ref{alg:cartesian-to-64bits}, which runs in
time $O(s)$. where $s$ is the size of a block.

\input{\AlgoPath/alg-cartesian-to-64bits-2e}

We answer a ranged maximum query from $l$ to $r$ as follows.  We
maintain the number of data that are relocated as $x$ and initialize
$x$ to 0.  We then loop through $t_l$ to $t_r$ and let the index run
from $l$ to $r$.  Every iteration adds 1 to $x$ then subtracts $t_j$
from $x$, and remember the index when $x$ becomes smaller or equal to
0.  We report the $j$, the {\em last} index when $x$ becomes smaller
or equal to 0, as the answer.  The pseudo code of the query answering
algorithm is in Algorithm~\ref{alg:cartesian64bits-query}.  The
correctness proof is in Theorem~\ref{thm:correctness}.  Note that all
the operations, e.g., shift, addition, subtraction, map directly to
machine instruction so that unlike Demaine's algorithm, our algorithm
is extremely to implement.

\input{\AlgoPath/alg-cartesian64bits-query-2e}

\begin{theorem} \label{thm:correctness}
  Algorithm~\ref{alg:cartesian64bits-query} correctly answers a ranged
  maximum query.
\end{theorem}
\begin{proof}
The algorithm will correctly answer a ranged maximum query for the
following reason.  When the $x$, the number of relocation, is smaller
than or equal to 0, that means the added data becomes the {\em root} of
the subtree of the query interval.  As ma result we should report the
{\em last time} the added data actually becomes the root of the
subtree, which according to the heap property, is the maximum in the
query interval.
\end{proof}

We choose the block size as 16 for performance.  Modern CPUs support
64 bit register and fast operation on them.  When we pack 16 $t_i$ in
to a 64 integer, we can leverage fast 64 bit instructions and improve
performance.  In addition, we do {\em not} build lookup tables since
implicitly maintain the Cartesian tree information within these 64
bit $t_i$, so we reduce the memory usage and improve cache
performance.

Our compressed sparse table approach does improve cache performance,
but will increase the time complexity.  The preprocessing time is
$O(n)$ time as in Fischer's algorithm.  However, a single query time
becomes $O(s)$, where $s$ is the block size.  This is acceptable in
practice since we use $s = 16$, which is a small constant.  The space
complexity is $O(n)$ as in Fischer's algorithm.  The overall time
complexity of the parallel version of our VGLCS algorithm becomes
$O(n^2 s / p + n \times \max(\log n, s))$.
