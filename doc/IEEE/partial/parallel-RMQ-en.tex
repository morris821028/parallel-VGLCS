\section{Parallel Range Maximum Query}
\label{sec:parallelRMQ}

\subsection{Background}

The efficiency of a parallel VGLCS algorithm is limited by the the
efficiency of its {\em range maximum query}.  We can use a parallel
sparse table implementation to answer range maximum queries with $p$
processors, so that it requires $O(n \log n / p + \log n)$ in
preprocessing, and takes only $O(1)$ time to answer a query.  On the
other hand, it is difficult to parallelize both pre-processing and
query on tree-like data structures efficiently.

Fischer~\cite{Fischer2006TheoreticalAP} consider a {\em blocked}
version of the sparse table.  The input is partitioned into $s$ blocks.
We now have two types of queries -- {\em super block} query and a {\em
  in-block} query.  A super block query queries the answer for
consecutive blocks, and a in-block query queries a segment {\em
  within} a block.  Then we compute the maximum for each blocks, and
compute a sparse table $T_s$ for these maximums.  It is easy to see
that we can query this sparse table $T_s$ {\em twice} to answer any
super block query.  An in-block queries can be answered by a single
lookup into an {\em index table} that encodes the elements in block
into a {\em Cartesian tree index}.  More details will be provided
later.  Since we can split {\em any} range query into at most a
super-block queries and two in-block queries, we need at most four
memory access to answer any ranged query.

Fischer's algorithm~\cite{Fischer2006TheoreticalAP} computes an {\em
  index table} for every block.  The algorithm scans through the data
within a block, and places the data within a maximum {\em Cartesian
  tree}.  One can think of this Cartesian tree as a heap where the
data are in heap order and the indexes of the data are in sorted
binary tree order.  As a result any ranged query from the $i$-th to
the $j$-th element is located at the least common ancestor in heap.
Let the index of this least common ancestor be $k$. then we add the
key value pairs $((i, j), k$ into the index table of this block.  Note
that the algorithm does {\em not} maintain the value the $k$-th
element.  Instead it keeps the {\em index} of the least common
ancestor so that two blocks with the same relative key order can share
the the same index table.  For example, the blocks $(10, 20, 30)$ and
$(101, 202, 303)$ can share the same index table in which $(1, 2)$ is
$2$.  That is the ranged maximum query from the first to the second
element will return the {\em second} element.  It is easy to see that
there are at most $C_s$ different index tables, where $C_s$ is $s$-th
Catalan number, i.e. the number of different Cartesian trees of $s$
nodes.  Also $C_s = \frac{1}{s+1}\binom{2s}{s} =
O(\frac{4^s}{s^{1.5}})$.  By knowing which Catalan index a block
belongs to, the algorithm can use the index table, which can be
identified by its Catalan index, and look into the correct index
table.

Fischer's algorithm~\cite{Fischer2006TheoreticalAP} builds index
tables by choosing $s = \frac{1}{4} \log n$ as the block size.  As a
result and preprocessing time is $O(n)$, and the space complexity is
$O(s^2 \frac{4^s}{s^{1.5}}) = O(n)$.  As a result a sequential version
requires $O(n)$ time in preprocessing, and $O(1)$ time in answering a
query, and both preprocessing time and query answering are the best.

Fischer's algorithm causes {\em serious cache miss} When the number of
data is large.  Fischer's algorithm will construct index tables for
blocks in a on-demand manner.  When the algorithm finds that the
corresponding index table is {\em not} present in memory, it will
build it in memory, which will be cached.  This process will repeat
the number of blocks times, and causes cache misses.

% 定義 B: 在兩層以上記憶體模型，較慢且大的儲存裝置會拆成好幾個區塊，每一個區塊會有 B 個元素。
% 假設要建立某個 index table，必然要反查原本的某一個 block (大小為 s)，一次會帶入 B 個連續片段。
% 當 s < B 時，反查操作有發生傳送次數大於 scan(n)

% Demaine's 那篇論文，採用直接建表的手法，不反查原本的輸入陣列 A
% 直接從將所有 block 索引壓縮成一個序列 S、在掃描 S 的過程中，得到所有二元樹的任意區間最大索引值

In order to reduce cache miss, Demaine~\cite{Demaine2009OnCT} proposed
{\em cache-aware} operations on Cartesian
tree~\cite{Vuillemin1980AUL}.  Therefore Demaine's algorithm skip the
checking for existing index table, and builds a index table for {\em
  every} block.  In addition, Demaine's algorithm uses a binary string
of length $2s$ as the identifier of a block using Cartesian tree.  One
can answer ranged maximum queries directly from examining this binary
string.  However, this operation requires counting the number of 1's
between the last two 0's, therefore it is hard to implement
efficiently in a modern computer.

We propose a new encoding for blocks.  We will use another
representation, called {\em compressed sparse table}, instead of the
binary string to improve performance.  The compressed sparse table is
inspired by Fischer's sparse table and Cartesian tree.

We replace lookup operation to naive operation.  The naive operation is
find the maximum value by comparing each element on the compressed data.
On the other hand, the lookup operation find the index of maximum value
from the index table. When loading a element from index table, it also
bring some useless data to caches.   In order to use cache efficiently,
the naive operation is better than the lookup operation because the
access pattern is almost one by one in our VGLCS algorithm.

We pick the fixed length $s = 16$ of each block, which can solve $n =
2^{64}$ one-dimension range maximum query.  When inserting $i$-th
elements, the number of $i$-th insertion satisfy $\sum_{i=1}^{n} l_i <
i$ where $0 \le l_i < s$ is the number of nodes remove from the
rightmost path of Cartesian tree.  Because all $l_i$ is small than 16,
it can present in a 4-bit integer.  Due to above property of Cartesian
tree, we merge 16 4-bit integers into a 64-bit integer to present a
Cartesian tree. The compressed algorithm~\ref{alg:cartesian-to-64bits}
run in $O(s)$.

% Morris: Further, tending to get less memory transfers is better than
% lookup table.  The compression version is easy-to-implement. But, The
% fixed size encoding is hard to mapping, so I run the RMQ with bit
% operation.

We proposed the intermediate algorithm in this section to improve the
cache miss in parallel environment.  We parallelize Fischer's
algorithm and reduce time complexity to $O(n / p + \log n)$ in
preprocessing, and $O(1)$ in query, Our algorithm combines compression
techniques from Demaine's paper, and reduces cache-miss and has a
asymptotically optimal time complexity.

\subsection{Compressed Cartesian Tree}

\iffalse 在 Fischer \cite{fischer} 的論文中，根據卡塔蘭數
$\frac{1}{s+1}\binom{2s}{s} = O(\frac{4^s}{s^{1.5}})$ 建立查找表
(lookup-table)，其中選擇 $s = \frac{1}{4} \log n$ 時，空間複雜度
$O(s^2 \frac{4^s}{s^{1.5}}) = o(n)$ 且建表複雜度 $o(n)$。每一個區間詢
問將會拆成 2 個 super-block 和 2 個 in-block 詢問，共計需要 4 次的記憶
體存取。在理論分析上，離線 RMQ 問題可在 $\theta(n)$ -- $\theta(1)$ 時
間內解決任一詢問。當 $n$ 越大時，這 4 次的記憶體存取會遭遇到嚴重的快取
未中 (cache miss)，在 Demaine ~\cite{demaine} 的論文中，發展出快取忘卻
(cache oblivious) 形式的查找方案，降低在離線版本中的 in-block 詢問產生
的快取未中。\fi


\iffalse在上述的技術中，我們可以藉由 Fischer 提出的方案平行化 RMQ 至
$O(n / p + \log n)$ -- $O(1)$，使用 Demaine 提供的技巧壓縮空間使用量，
降低快取未中以提升運行效能。這裡我們挑選固定長度的壓縮方案 $s = 16$，
其能解決序列長度為 $n = 2^{64}$ 的區間查找，將 16 個整數壓縮成一棵笛卡
爾樹。在第 $i$ 次插入時，左旋的次數 $l_i$，每次操作皆符合
$\sum_{i=1}^{n} l_i < i$。\fi



\input{algorithms/alg-cartesian-to-64bits}

Finally, the appropriate size can compress the usage of space to
reduce cache miss and also show better performance in modern 64-bit
register.  We modify Demaine's range query algorithm as the algorithm
\ref{alg:cartesian64bits-query}.

\input{algorithms/alg-cartesian64bits-query}

\iffalse
因所有 $l_i < 16$，使得每個 $l_i$ 可用 4-bit 表示之，
整體便可用 64-bit 長整數表示一棵笛卡爾樹的狀態。
為了現在常見的 64-byte 快取列 (cache line) 和 64-bit 暫存器 (register) 考量，
我們選用合適的大小進行測試，不僅壓縮空間使用量，同時也減少快取未中的問題。
最後，我們得到壓縮算法 \ref{alg:cartesian-to-64bits}，其相對應的區間查找算法，
根據 Demaine \cite{demaine} 進行修改，得到壓縮下的詢問算法 \ref{alg:cartesian64bits-query}。
\fi

In VGLCS problem, above algorithm provide compression skill to reduce
cache miss, but increase the time complexity.  The preprocessing spend
$O(n)$ time, single query spends $O(s)$ time in RMQ, and the space
complexity is  $O(n)$.  Totally, time complexity of the VGLCS algorithm
is $O(n^2 \; s / p + n \max(\log n, s))$.

\iffalse
回到 VGLCS 的應用中，上述算法使用壓縮方式降低快取未中。
我們可以使用上述的算法取代原先的并查集，建表的時間複雜度為 $O(n)$，
單一查詢的時間複雜度為 $O(s)$。
整體的時間複雜度為 $O(n^2 \; s / p + n \max(\log n, s))$。
\fi
