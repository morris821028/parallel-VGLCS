\section{Ranged Maximum Query} \label{sec:parallelRMQ}

% Why Ranged Maximum Query is important?

Form our simple version of VGLCS algorithm
(Algorithm~\ref{alg:parallel-VGLCS}) we observe that the efficiency of
a parallel VGLCS algorithm depends on the the efficiency of its {\em
  ranged maximum query}.  We can use a parallel sparse table
implementation to answer ranged maximum queries with $p$ processors,
so that it requires $O(n \log n / p + \log n)$ in preprocessing, and
takes only $O(1)$ time to answer a query.  On the other hand, it is
difficult to efficiently parallelize the querying on tree-like data
structures, e.g., disjoint sets.

\subsection{Blocked Sparse Table}

\begin{figure}[!thb]   \label{fig:interval-decomposition}
  \centering 
    \includegraphics[width=0.9\linewidth]{\GraphicPath/fig-interval-decomposition.pdf}

    \includegraphics[width=0.9\linewidth]{\GraphicPath/fig-sparse-table.pdf}
  \caption{A Sparse Table}
\end{figure}

We will further improve the efficiency of our parallel VGLCS algorithm
by a improved sparse table implementation.
Fischer~\cite{Fischer2006TheoreticalAP} consider a {\em blocked}
version of the sparse table.  The algorithm first partition the data
into $s$ blocks, then it computes the maximum of each block, and
compute a sparse table $T_s$ for these maximums.

The blocked approach answers a ranged maximum query as follows.  We
consider two types of queries -- {\em super block} query and {\em
  in-block} query.  A super block query queries the answer for
consecutive blocks, and a in-block query queries a segment {\em
  within} a block.  It is easy to see that we can answer a super block
query by querying $T_s$ at most {\em twice}.  We can also answer an
in-block queries answered by a {\em single} lookup into an {\em index
  table} that encodes the elements in block into a {\em Cartesian tree
  index}.  We will provide more details on this later.  Since we can
split {\em any} ranged query into at most {\em two} queries into $T_s$
and {\em two} in-block queries, we need at most {\em four} memory
access to answer any ranged maximum query.


\subsubsection{In-block Query}

Fischer's algorithm~\cite{Fischer2006TheoreticalAP} scans through the
data within a block, and places the data into a {\em Cartesian tree}.
Each node of the Cartesian tree has a data and the index of this data
in the block.  One can think of this Cartesian tree as a heap where
the data are in {\em heap order}, and the indexes of the data are in
{\em sorted binary search tree order}.  Please refer to
Figure~\ref{fig:ancesstor-cartesian} for an illustration.  As a result
the answer to the in-block ranged maximum query from the $i$-th to the
$j$-th element of a block is located at their {\em least common
  ancestor} in the tree.

\begin{figure}[htbp]   
  \centering
  \includegraphics[width=\linewidth]{\GraphicPath/fig-interval-cartesian.pdf}
  \caption{Ancestor table}
  \label{fig:ancesstor-cartesian}
\end{figure}

Fischer's algorithm computes an {\em ancestor table} for every block.
After scanning the data in a block, the algorithm will build a
Cartesian tree, and is able to answer the in-block ranged maximum
query from the $i$-th to the $j$-th element.  Let the index of this
least common ancestor be $k$. then the algorithm add the key value
pairs $((i, j), k)$ into the ancestor table of this block.  Please
refer to Figure~\ref{fig:ancesstor-cartesian} for an illustration.
That is, one can think of the ancestor table as a mapping table from
the in-block query $(i, j)$ to its answer $k$.  Note that the
algorithm does {\em not} maintain the value the $k$-th element.
Instead it keeps the {\em index}, i.e., $k$, of the least common
ancestor so that two blocks with the {\em same relative key order} can
{\em share} an ancestor table.  For example, the first three blocks in
Figure~\ref{fig:ancesstor-cartesian} can share the same ancestor table
because they have their same Cartesian tree.  Consequently an in-block
ranged maximum query $(1, 3)$ to {\em any} of these three blocks will
return the {\em same} answer $2$.

The main idea of Fischer's algorithm is to compute an ancestor table
for each block, and answer an in-block query directly by looking into
its ancestor table.  It is easy to see that there are $C_s$ different
Cartesian trees, where $C_s$ is number of different binary trees of
$s$ nodes.  It is also easy to see that each block can be identified
by the shape of its Cartesian tree, so it can be represented by an
index.  For ease of notation we will refer to this index as its {\em
  Catalan index}.  By knowing the Catalan index of a block, we can
answer a in-block ranged maximum query by looking into its
corresponding ancestor table.  Please refer to
Figure~\ref{fig:ancesstor-cartesian} for an illustration.

\subsubsection{Cache Issues}

Fischer's algorithm~\cite{Fischer2006TheoreticalAP} builds ancestor
tables by choosing $s = \frac{1}{4} \log n$ as the block size.  Recall
that $C_s = \frac{1}{s+1}\binom{2s}{s} = O(\frac{4^s}{s^{1.5}})$.  As
a result the preprocessing time is $O(n)$, and the space complexity is
$O(s^2 \frac{4^s}{s^{1.5}}) = O(n)$.  That is, a sequential version
requires $O(n)$ time in preprocessing, and $O(1)$ time in answering a
query, and both preprocessing time and query answering are the best.

Fischer's algorithm causes {\em serious cache miss} when the number of
data is large.  Fischer's algorithm will construct ancestor tables for
blocks in an on-demand manner.  When the algorithm finds that the
corresponding ancestor table is {\em not} present in memory, it will
build it in memory, which will be cached.  This process will repeat
the number of blocks times, and causes serious cache misses.

In order to reduce cache miss, Demaine~\cite{Demaine2009OnCT} proposed
{\em cache-aware} operations on Cartesian
tree~\cite{Vuillemin1980AUL}.  Demaine's algorithm skips the checking
for existing ancestor table, and builds an ancestor table for {\em
  every} block.  In addition, Demaine's algorithm uses a binary string
of length $2s$ as an identifier of a block to its Cartesian tree.  The
binary string is encoded in such a way that one can answer an in-block
ranged maximum query by examining this binary string.  However, this
operation requires counting the number of 1's {\em between} the last
two 0's, which is {\em hard} to implement efficiently in a modern
computer.

% We replace lookup operation to naive operation.  The naive operation
% is find the maximum value by comparing each element on the compressed
% data. On the other hand, the lookup operation find the index of
% maximum value from the ancestor table. When loading a element from index
% table, it also bring some useless data to caches.   In order to use
% cache efficiently, the naive operation is better than the lookup
% operation because the access pattern is almost one by one in our VGLCS
% algorithm.

\subsection{Compressed Cartesian Tree} \label{sec:cct}

We propose a new encoding for blocks called {\em compressed sparse
  table}, instead of the binary string, in order to improve
performance.  The compressed sparse table is inspired by Fischer's
sparse table and the Cartesian tree.  We also use the block approach
and select a fixed block size $s = {{\frac{\log n}{4}}} = 16$, which
can efficiently answer one-dimension ranged maximum query for up to $n
= 2^{64}$ data.

The compressed sparse table works by keeping only the {\rm rightmost}
path of the Cartesian tree.  Consider the processing of adding a data.
Let the $i$-th added data be $a_i$, so we need to {\em relocate} all
the data along the rightmost path of the Cartesian tree that are
smaller than $a_i$.  Let $t_i$ be the number of nodes that need to be
relocated, and it is easy to see that $0 \le t_i < s$.

\begin{figure}[!thb]
  \centering
  \includegraphics[width=\linewidth]{\GraphicPath/fig-cartesian-encoding-stack.pdf}
  \caption{Right most path in stack}
  \label{fig:interval-cartesian}
\end{figure}

The key point of compressed sparse table is that we store $t_i$ to
{\em implicitly identify} this Cartesian tree for this block of data,
so that we can answer queries by examining these $t_i$'s.  More
details on how to answer queries with these $t_i$ will be described
later.  Now since all $t_i$'s are small than 16, we can represent each
of them as a 4-bit integer.  We then merge 16 of these 4-bit integers
into a 64-bit integer to present a Cartesian tree for a block.  The
pseudo code on how to build a 64 bit integer to represent a block of
16 data is in Algorithm~\ref{alg:cartesian-to-64bits}, which runs in
time $O(s)$. where $s$ is the size of a block.

\input{\AlgoPath/alg-cartesian-to-64bits-2e}

We answer a ranged maximum query from $l$ to $r$ as follows.  We
maintain the number of data that are relocated as $x$ and initialize
$x$ to 0.  We then loop through $t_l$ to $t_r$ and let the index run
from $l$ to $r$.  Every iteration adds 1 to $x$ then subtracts $t_j$
from $x$, and remember the index when $x$ becomes smaller or equal to
0.  We report the $j$, the {\em last} index when $x$ becomes smaller
or equal to 0, as the answer.  The pseudo code of the query answering
algorithm is in Algorithm~\ref{alg:cartesian64bits-query}.  The
correctness proof is in Theorem~\ref{thm:correctness}.  Note that all
the operations, e.g., shift, addition, subtraction, map directly to
machine instruction so that unlike Demaine's algorithm, our algorithm
is extremely to implement.

\input{\FormulaPath/fun-rmq.tex}

\input{\AlgoPath/alg-cartesian64bits-query-2e}

\begin{theorem} \label{thm:correctness}
  Algorithm~\ref{alg:cartesian64bits-query} correctly answers a ranged
  maximum query.
\end{theorem}
\begin{proof}
The algorithm will correctly answer a ranged maximum query for the
following reason.  When the $x$, the number of relocation, is smaller
than or equal to 0, that means the added data becomes the {\em root} of
the subtree of the query interval.  As ma result we should report the
{\em last time} the added data actually becomes the root of the
subtree, which according to the heap property, is the maximum in the
query interval.
\end{proof}

We choose the block size as 16 for performance.  Modern CPUs support
64 bit register and fast operation on them.  When we pack 16 $t_i$ in
to a 64 integer, we can leverage fast 64 bit instructions and improve
performance.  In addition, we do {\em not} build lookup tables since
implicitly maintain the Cartesian tree information within these 64
bit $t_i$, so we reduce the memory usage and improve cache
performance.

Our compressed sparse table approach does improve cache performance,
but will increase the time complexity.  The preprocessing time is
$O(n)$ time as in Fischer's algorithm.  However, a single query time
becomes $O(s)$, where $s$ is the block size.  This is acceptable in
practice since we use $s = 16$, which is a small constant.  The space
complexity is $O(n)$ as in Fischer's algorithm.  The overall time
complexity of the parallel version of our VGLCS algorithm becomes
$O(n^2 s / p + n \times \max(\log n, s))$.
