\section{Parallel Range Maximum Query}
\label{sec:parallelRMQ}

\subsection{Background}

The efficiency of a parallel VGLCS algorithm is limited by the the
efficiency of its {\em range maximum query}.  We can use a parallel
sparse table implementation to answer range maximum queries with $p$
processors, so that it requires $O(n \log n / p + \log n)$ in
preprocessing, and takes only $O(1)$ time to answer a query.  On the
other hand, it is difficult to parallelize both pre-processing and
query on tree-like data structures efficiently.

Fischer~\cite{Fischer2006TheoreticalAP} consider a {\em blocked}
version of the sparse table.  The input is partitioned into $s$ blocks.
We now have two types of queries -- {\em super block} query and a {\em
  in-block} query.  A super block query queries the answer for
consecutive blocks, and a in-block query queries a segment {\em
  within} a block.  Then we compute the maximum for each blocks, and
compute a sparse table $T_s$ for these maximums.  It is easy to see
that we can query this sparse table $T_s$ {\em twice} to answer any
super block query.  An in-block queries can be answered by a single
lookup into an {\em index table} that encodes the elements in block
into a {\em Cartesian tree index}.  More details will be provided
later.  Since we can split {\em any} range query into at most a
super-block queries and two in-block queries, we need at most four
memory access to answer any ranged query.

Fischer's algorithm~\cite{Fischer2006TheoreticalAP} computes an {\em
  index table} for every block.  The algorithm scans through the data
within a block, and places the data within a maximum {\em Cartesian
  tree}.  One can think of this Cartesian tree as a heap where the
data are in heap order and the indexes of the data are in sorted
binary tree order.  As a result any ranged query from the $i$-th to
the $j$-th element is located at the least common ancestor in heap.
Let the index of this least common ancestor be $k$. then we add the
key value pairs $((i, j), k$ into the index table of this block.  Note
that the algorithm does {\em not} maintain the value the $k$-th
element.  Instead it keeps the {\em index} of the least common
ancestor so that two blocks with the same relative key order can share
the the same index table.  For example, the blocks $(10, 20, 30)$ and
$(101, 202, 303)$ can share the same index table in which $(1, 2)$ is
$2$.  That is the ranged maximum query from the first to the second
element will return the {\em second} element.  It is easy to see that
there are at most $C_s$ different index tables, where $C_s$ is $s$-th
Catalan number, i.e. the number of different Cartesian trees of $s$
nodes.  Also $C_s = \frac{1}{s+1}\binom{2s}{s} =
O(\frac{4^s}{s^{1.5}})$.  By knowing which Catalan index a block
belongs to, the algorithm can use the index table, which can be
identified by its Catalan index, and look into the correct index
table.

Fischer's algorithm~\cite{Fischer2006TheoreticalAP} builds index
tables by choosing $s = \frac{1}{4} \log n$ as the block size.  As a
result and preprocessing time is $O(n)$, and the space complexity is
$O(s^2 \frac{4^s}{s^{1.5}}) = O(n)$.  As a result a sequential version
requires $O(n)$ time in preprocessing, and $O(1)$ time in answering a
query, and both preprocessing time and query answering are the best.

% Morris: Demaine, On Cartesian Trees and Range Minimum Queries

When $n$ is large, Fischer's algorithm uses four memory accesses,
causing {\em serious cache miss}.  In order to reduce cache miss,
Demaine~\cite{Demaine2009OnCT} introduced cache-oblivious operations
on Cartesian tree~\cite{Vuillemin1980AUL}.

% intermediate ???

% Definition: the signature of the block = the index table of the block
% Definition: all the signature of the blocks in an array $A$ = $S(A)$

% Morris: In Demaine's paper, "In the cache-oblivious model however,
% each of the n/s blocks checks might bring a new block to cache. If the
% block size $s$ < cache size $B$, this incurs more memory transfers".
% Due to Fischer's algorithm will construct the part of the index table
% $P$ correspond to signatures in $S(A)$.

% Morris: In order to avoid memory transfers, building the entire table
% $P$ is the good solution.  In the Demaine's paper, it use binary
% strings to present a block, and the length of binary string is $2s$.

% Morris: the binary string format need "count the number of 1's between
% the last two 0's".  It is hard to implement efficiently in modern
% computer.  Therefore, I use fixed size to present it.

% Morris: Further, tending to get less memory transfers is better than
% lookup table.  The compression version is easy-to-implement. But, The
% fixed size encoding is hard to mapping, so I run the RMQ with bit
% operation.

We proposed the intermediate algorithm in this section to improve the
cache miss in parallel environment.  We parallelize Fischer's
algorithm and reduce time complexity to $O(n / p + \log n)$ in
preprocessing, and $O(1)$ in query, Our algorithm combines compression
techniques from Demaine's paper, and reduces cache-miss and has a
asymptotically optimal time complexity.

\subsection{Compressed Cartesian Tree}

\iffalse 在 Fischer \cite{fischer} 的論文中，根據卡塔蘭數
$\frac{1}{s+1}\binom{2s}{s} = O(\frac{4^s}{s^{1.5}})$ 建立查找表
(lookup-table)，其中選擇 $s = \frac{1}{4} \log n$ 時，空間複雜度
$O(s^2 \frac{4^s}{s^{1.5}}) = o(n)$ 且建表複雜度 $o(n)$。每一個區間詢
問將會拆成 2 個 super-block 和 2 個 in-block 詢問，共計需要 4 次的記憶
體存取。在理論分析上，離線 RMQ 問題可在 $\theta(n)$ -- $\theta(1)$ 時
間內解決任一詢問。當 $n$ 越大時，這 4 次的記憶體存取會遭遇到嚴重的快取
未中 (cache miss)，在 Demaine ~\cite{demaine} 的論文中，發展出快取忘卻
(cache oblivious) 形式的查找方案，降低在離線版本中的 in-block 詢問產生
的快取未中。\fi


\iffalse在上述的技術中，我們可以藉由 Fischer 提出的方案平行化 RMQ 至
$O(n / p + \log n)$ -- $O(1)$，使用 Demaine 提供的技巧壓縮空間使用量，
降低快取未中以提升運行效能。這裡我們挑選固定長度的壓縮方案 $s = 16$，
其能解決序列長度為 $n = 2^{64}$ 的區間查找，將 16 個整數壓縮成一棵笛卡
爾樹。在第 $i$ 次插入時，左旋的次數 $l_i$，每次操作皆符合
$\sum_{i=1}^{n} l_i < i$。\fi

The compressed sparse table is composed of Fischer's sparse table and
Cartesian tree, and we replace lookup operation to naive operation. We
pick the fixed length $s = 16$ of each block, which can solve $n =
2^{64}$ one-dimension range maximum query.  When inserting $i$-th
elements, the number of $i$-th insertion satisfy $\sum_{i=1}^{n} l_i <
i$ where $0 \le l_i < s$ is the number of nodes remove from the
rightmost path of Cartesian tree.  Because all $l_i$ is small than 16,
it can present in a 4-bit integer.  Due to above property of Cartesian
tree, we merge 16 4-bit integers into a 64-bit integer to present a
Cartesian tree. The compressed algorithm \ref{alg:cartesian-to-64bits}
run in $O(s)$.


\input{algorithms/alg-cartesian-to-64bits}

Finally, the appropriate size can compress the usage of space to
reduce cache miss and also show better performance in modern 64-bit
register.  We modify Demaine's range query algorithm as the algorithm
\ref{alg:cartesian64bits-query}.

\input{algorithms/alg-cartesian64bits-query}

\iffalse
因所有 $l_i < 16$，使得每個 $l_i$ 可用 4-bit 表示之，
整體便可用 64-bit 長整數表示一棵笛卡爾樹的狀態。
為了現在常見的 64-byte 快取列 (cache line) 和 64-bit 暫存器 (register) 考量，
我們選用合適的大小進行測試，不僅壓縮空間使用量，同時也減少快取未中的問題。
最後，我們得到壓縮算法 \ref{alg:cartesian-to-64bits}，其相對應的區間查找算法，
根據 Demaine \cite{demaine} 進行修改，得到壓縮下的詢問算法 \ref{alg:cartesian64bits-query}。
\fi

In VGLCS problem, above algorithm provide compression skill to reduce
cache miss, but increase the time complexity.  The preprocessing spend
$O(n)$ time, single query spends $O(s)$ time in RMQ, and the space
complexity is  $O(n)$.  Totally, time complexity of the VGLCS algorithm
is $O(n^2 \; s / p + n \max(\log n, s))$.

\iffalse
回到 VGLCS 的應用中，上述算法使用壓縮方式降低快取未中。
我們可以使用上述的算法取代原先的并查集，建表的時間複雜度為 $O(n)$，
單一查詢的時間複雜度為 $O(s)$。
整體的時間複雜度為 $O(n^2 \; s / p + n \max(\log n, s))$。
\fi
