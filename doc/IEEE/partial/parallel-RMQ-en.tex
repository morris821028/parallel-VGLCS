\section{Parallel Ranged Maximum Query}
\label{sec:parallelRMQ}

\subsection{Background}

The efficiency of a parallel VGLCS algorithm is limited by the the
efficiency of its {\em ranged maximum query}.  We can use a parallel
sparse table implementation to answer ranged maximum queries with $p$
processors, so that it requires $O(n \log n / p + \log n)$ in
preprocessing, and takes only $O(1)$ time to answer a query.  On the
other hand, it is difficult to parallelize both pre-processing and
query on tree-like data structures efficiently.

Fischer~\cite{Fischer2006TheoreticalAP} consider a {\em blocked}
version of the sparse table.  The input is partitioned into $s$ blocks.
We now have two types of queries -- {\em super block} query and a {\em
  in-block} query.  A super block query queries the answer for
consecutive blocks, and a in-block query queries a segment {\em
  within} a block.  Then we compute the maximum for each blocks, and
compute a sparse table $T_s$ for these maximums.  It is easy to see
that we can query this sparse table $T_s$ {\em twice} to answer any
super block query.  An in-block queries can be answered by a single
lookup into an {\em index table} that encodes the elements in block
into a {\em Cartesian tree index}.  More details will be provided
later.  Since we can split {\em any} ranged query into at most a
super-block queries and two in-block queries, we need at most four
memory access to answer any ranged query.

Fischer's algorithm~\cite{Fischer2006TheoreticalAP} computes an {\em
  index table} for every block.  The algorithm scans through the data
within a block, and places the data within a maximum {\em Cartesian
  tree}.  One can think of this Cartesian tree as a heap where the
data are in heap order and the indexes of the data are in sorted
binary tree order.  As a result any ranged query from the $i$-th to
the $j$-th element is located at the least common ancestor in heap.
Let the index of this least common ancestor be $k$. then we add the
key value pairs $((i, j), k$ into the index table of this block.  Note
that the algorithm does {\em not} maintain the value the $k$-th
element.  Instead it keeps the {\em index} of the least common
ancestor so that two blocks with the same relative key order can share
the the same index table.  For example, the blocks $(10, 20, 30)$ and
$(101, 202, 303)$ can share the same index table in which $(1, 2)$ is
$2$.  That is the ranged maximum query from the first to the second
element will return the {\em second} element.  It is easy to see that
there are at most $C_s$ different index tables, where $C_s$ is $s$-th
Catalan number, i.e. the number of different Cartesian trees of $s$
nodes.  Also $C_s = \frac{1}{s+1}\binom{2s}{s} =
O(\frac{4^s}{s^{1.5}})$.  By knowing which Catalan index a block
belongs to, the algorithm can use the index table, which can be
identified by its Catalan index, and look into the correct index
table.

Fischer's algorithm~\cite{Fischer2006TheoreticalAP} builds index
tables by choosing $s = \frac{1}{4} \log n$ as the block size.  As a
result and preprocessing time is $O(n)$, and the space complexity is
$O(s^2 \frac{4^s}{s^{1.5}}) = O(n)$.  As a result a sequential version
requires $O(n)$ time in preprocessing, and $O(1)$ time in answering a
query, and both preprocessing time and query answering are the best.

Fischer's algorithm causes {\em serious cache miss} When the number of
data is large.  Fischer's algorithm will construct index tables for
blocks in a on-demand manner.  When the algorithm finds that the
corresponding index table is {\em not} present in memory, it will
build it in memory, which will be cached.  This process will repeat
the number of blocks times, and causes cache misses.

In order to reduce cache miss, Demaine~\cite{Demaine2009OnCT} proposed
{\em cache-aware} operations on Cartesian
tree~\cite{Vuillemin1980AUL}.  Therefore Demaine's algorithm skip the
checking for existing index table, and builds a index table for {\em
  every} block.  In addition, Demaine's algorithm uses a binary string
of length $2s$ as the identifier of a block using Cartesian tree.  One
can answer ranged maximum queries directly from examining this binary
string.  However, this operation requires counting the number of 1's
between the last two 0's, therefore it is {\em hard} to implement
efficiently in a modern computer.

% We replace lookup operation to naive operation.  The naive operation
% is find the maximum value by comparing each element on the compressed
% data. On the other hand, the lookup operation find the index of
% maximum value from the index table. When loading a element from index
% table, it also bring some useless data to caches.   In order to use
% cache efficiently, the naive operation is better than the lookup
% operation because the access pattern is almost one by one in our VGLCS
% algorithm.

\subsection{Compressed Cartesian Tree} \label{sec:cct}

We propose a new encoding for blocks.  We a {\em compressed sparse
  table}, instead of the binary string, in order to improve
performance.  The compressed sparse table is inspired by Fischer's
sparse table and the Cartesian tree.  We select a fixed block size of
$s = {{\frac{\log n}{4}}} = 16$, which can efficiently answer
one-dimension ranged maximum query for up to $n = 2^{64}$ data.  We
consider the processing of adding data.  Let the $i$-th added data be
$a_i$, so we need to {\em relocate} all the data along the {\em
  rightmost} path of the Cartesian tree that are smaller than $a_i$.
Let $t_i$ be the number of nodes that need to be relocated, and it
is easy to see that $0 \le t_i < s$.  The key point is that we store
these $t_i$ to {\em implicitly identify} this Cartesian tree for this
block of data, so that we can answer queries by examining these $t_i$.
More details on how to answer queries with these $t_i$ will be
described later.  Now since all $t_i$'s are small than 16, we can
represent each of them as a 4-bit integer.  We then merge 16 of these
4-bit integers into a 64-bit integer to present a Cartesian tree for a
block.  The pseudo code on how to build a 64 bit integer to represent
a block of 16 data is in Algorithm~\ref{alg:cartesian-to-64bits},
which runs in time $O(s)$. where $s$ is the size of a block.

\input{algorithms/alg-cartesian-to-64bits}

We answer a ranged maximum query from $l$ to $r$ as follows.  We
maintain the number of data that are relocated as $x$ and initialize
$x$ to 0.  We then loop through $t_l$ to $t_r$ and let the index run
from $l$ to $r$.  Every iteration adds 1 to $x$ then subtracts $t_j$
from $x$, and remember the index when $x$ becomes smaller or equal to
0.  We report the $j$, the {\em last} index when $x$ becomes smaller
or equal to 0, as the answer.  The pseudo code of the query answering
algorithm is in Algorithm~\ref{alg:cartesian64bits-query}.  The
corecteness proof is in Theorem~\ref{thm:correctness}.  Note that all
the operations, e.g., shift, addtion, subtraction, map directly to
machine instrcution so that unlike Demaine's algorithm, our algorithm
is extremely to implement.

\input{algorithms/alg-cartesian64bits-query}

\begin{theorem} \label{thm:correctness}
  Algorithm~\ref{alg:cartesian64bits-query} correctly answers a ranged
  maximum query.
\end{theorem}
\begin{proof}
The algorithm will correctly answer a ranged maximum query for the
following reason.  When the $x$, the number of relocation, is smaller
than or equal to 0, that means the added data becomes the {\em root} of
the subtree of the query interval.  As ma result we should report the
{\em last time} the added data actually becomes the root of the
subtree, which according to the heap property, is the maximum in the
query interval.
\end{proof}

We choose the block size as 16 for performance.  Modern CPUs support
64 bit register and fast operation on them.  When we pack 16 $t_i$ in
to a 64 integer, we can leverage fast 64 bit instructions and improve
performance.  In addition, we do {\em not} build lookup tables since
implicitly maintain the Cartesian tree information within these 64
bit $t_i$, so we reduce the memory usage and improve cache
performance.

\iffalse 因所有 $t_i < 16$，使得每個 $l_i$ 可用 4-bit 表示之，整體便可
用 64-bit 長整數表示一棵笛卡爾樹的狀態。為了現在常見的 64-byte 快取列
(cache line) 和 64-bit 暫存器 (register) 考量，我們選用合適的大小進行
測試，不僅壓縮空間使用量，同時也減少快取未中的問題。最後，我們得到壓縮
算法 \ref{alg:cartesian-to-64bits}，其相對應的區間查找算法，根據
Demaine \cite{demaine} 進行修改，得到壓縮下的詢問算法
\ref{alg:cartesian64bits-query}。\fi

Our VGLCS algorithm using the compressed sparse table does improve
cache per romance, but increase the time complexity.  The
preprocessing time is $O(n)$ time as in Fischer's algorithm.  However,
a single query time becomes $O(s)$, where $s$ is the block size.  This
is acceptable in practice since we use $s = 16$, which is a small
constant.  The space complexity is $O(n)$ as in Fischer's algorithm.
The overall time complexity of the parallel version of our VGLCS
algorithm is $O(n^2 s / p + n \times \max(\log n, s))$.

\iffalse
回到 VGLCS 的應用中，上述算法使用壓縮方式降低快取未中。
我們可以使用上述的算法取代原先的并查集，建表的時間複雜度為 $O(n)$，
單一查詢的時間複雜度為 $O(s)$。
整體的時間複雜度為 $O(n^2 \; s / p + n \max(\log n, s))$。
\fi
