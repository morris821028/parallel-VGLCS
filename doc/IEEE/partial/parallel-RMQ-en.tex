\section{Range Maximum Query} \label{sec:parallelRMQ}

In this section, we will describe our approach to address the
challenges in the second stage of Algorithm~\ref{alg:parallel-VGLCS},
i.e., an efficient {\em range maximum query}.  The range maximum query
problem is more complicated than the previous incremental suffix
maximum query problem because the suffix maximum query is a special
case of the range maximum query.

The operations to support range maximum query is similar to those for
suffix maximum query.  A {\sc make} operation creates an empty array
$A$, an {\sc Append(V)} operation appends a value $V$ to the end of an
array $A$.  Finally, a {\sc Query(L, R)} operation finds the {\em
  maximum} value among the $L$-th value to the $R$-th value of an
array $A$.

%% We can use a parallel sparse table implementation to answer range
%% maximum queries with $p$ processors, so that it requires $O(n \log n /
%% p + \log n)$ in preprocessing, and takes only $O(1)$ time to answer a
%% query.  On the other hand, it is difficult to efficiently parallelize
%% the querying on tree-like data structures, e.g., disjoint sets.

\subsection{Blocked Sparse Table} \label{sec:blocked-sparse-table}

\begin{figure}[!thb]
  \centering \subfigure[Array] {
    \includegraphics[width=0.65\linewidth]{\GraphicPath/fig-interval-decomposition.pdf}
  } \subfigure[Blocked sparse table] {
    \includegraphics[width=0.25\linewidth]{\GraphicPath/fig-sparse-table.pdf}
  }
  \caption{A Sparse Table}
  \label{fig:block-interval-decomposition}
\end{figure}

We improve the efficiency of our parallel VGLCS algorithm with a {\em
  blocked} sparse table proposed by
Fischer~\cite{Fischer2006TheoreticalAP}.  The blocked approach first
partitions the data into blocks of size $s$, then it computes the
maximum of each block, and compute a sparse table $T_s$ for these
maximums.  Recall that the {\em unblocked} sparse table in
Section~\ref{sec:sparse-table} does {\em not} partition data into
blocks, but builds a table of $\log n$ rows of maximum for different
lengths of intervals.

The blocked approach answers a range maximum query as follows.  We
consider two types of queries -- {\em super block} query and {\em
  in-block} query.  A super block query queries the answer for {\em
  consecutive} blocks, and an in-block query queries a segment {\em
  within} a block.  It is easy to see that we can answer a super block
query by querying $T_s$ at most {\em twice}, as described in
Section~\ref{sec:sparse-table}.  We can also answer an in-block
queries answered by a {\em single} lookup into an {\em least common
  ancestor table}.  We will provide more details on this table later.
Since we can split {\em any} range query into at most {\em two}
queries into $T_s$ and {\em two} in-block queries, we need at most
{\em four} memory access to answer any range maximum query.  Also
since the super block query is easy to answer with $T_s$, we will now
focus on in-block query.

Fischer's algorithm~\cite{Fischer2006TheoreticalAP} scans through the
data within a block and places them into a {\em Cartesian tree}.  Each
node of this Cartesian tree stores a data and the index of this data
within the block.  One can think of this Cartesian tree as a {\em heap}
where the data are in heap order, and the indexes of the data are in
{\em sorted binary search tree order}.  Please refer to
Figure~\ref{fig:ancesstor-cartesian} for an illustration.  Note that in
Figure~\ref{fig:ancesstor-cartesian} the horizontal position of tree
nodes reflects their position in the data block.  As a result, the
answer to an in-block range maximum query from the $i$-th to the $j$-th
element of a block is located at their {\em least common ancestor} in
the Cartesian tree.  For example, the maximum from the fourth (with data
3) to the six elements (with data 2) is located at the fifth element
(with data 7).

\begin{figure}[!thb]   
  \centering
  \includegraphics[width=0.9\linewidth]{\GraphicPath/fig-interval-cartesian.pdf}
  \caption{Least common ancestor tables}
  \label{fig:ancesstor-cartesian}
\end{figure}

To answer in-block queries, Fischer's algorithm computes a {\em least
common ancestor table} for every block.  After scanning the data in a
block, Fischer's algorithm builds a Cartesian tree and its least common
ancestor (LCA) table.  An LCA table stores all $((i, j), k)$'s where the
$k$-th data is the common ancestor for the $i$-th and $j$-th data in a
block.  Please refer to Figure~\ref{fig:ancesstor-cartesian} for an
illustration.  Now we can answer an in-block range maximum query from
the $i$-th to the $j$-th element simply by look into the LCA table and
return $k$,  least common ancestor table of this block.  One can think
of the LCA table as a mapping table from an in-block query $(i, j)$ to
its answer $k$.

Note that the algorithm does {\em not} maintain the value of the $k$-th
element.  Instead, it stores the {\em index}, i.e., $k$, of the least
common ancestor so that two blocks with the {\em same relative key
order} can {\em share} a least common ancestor table.  For example, the
first three blocks in Figure~\ref{fig:ancesstor-cartesian} can share the
same LCA table because they have the same Cartesian tree.  Consequently,
an in-block range maximum query $(1, 3)$ to {\em any} of these three
blocks will return the {\em same} answer $2$.

The main idea of Fischer's algorithm is to compute an LCA table for
every block, and answer an in-block query directly by looking into its
LCA table.  It is easy to see that there are ${\cal C}_s$ different Cartesian
trees, where ${\cal C}_s$ is number of different binary trees of $s$ nodes. It
is also easy to see that each block can be identified by the shape of
its Cartesian tree, so it can be represented by an index.  For ease of
notation we will refer to this index as its {\em Catalan index}. By
knowing the Catalan index of a block, we can answer an in-block range
maximum query by looking into its corresponding LCA table. Please refer
to Figure~\ref{fig:ancesstor-cartesian} for an illustration.

Fischer's algorithm~\cite{Fischer2006TheoreticalAP} builds least
ancestor tables by choosing $s = \frac{\log n}{4}$ as the block size
for performance reason.  Recall that $C_s = \frac{1}{s+1}\binom{2s}{s}
= O(\frac{4^s}{s^{1.5}})$.  As a result the time to scan data and to
build Cartesian tree and LCA tables is $O(n)$, and the space
requirement is $O(s^2 \frac{4^s}{s^{1.5}}) = O(n)$.  That is, a
sequential Fischer's algorithm requires $O(n)$ time in preprocessing,
and $O(1)$ time to answer a query.  It is easy to see that both
preprocessing time and query answering are optimal.

One drawback of Fischer's algorithm is that it causes {\em serious
  cache miss} when the number of data is large.  Fischer's algorithm
will construct LCA tables for blocks {\em on-demand}.  When the
algorithm finds that the corresponding LCA table is {\em not} in
memory, it will build the LCA table, which will be cached and this may
{\em evict} other LCA tables from cache.  This LCA table building
process repeats for as many times as the number of blocks, and may
cause serious cache misses.

In order to reduce cache miss, Demaine~\cite{Demaine2009OnCT} proposed
{\em cache-aware} operations on Cartesian
tree~\cite{Vuillemin1980AUL}.  Demaine's algorithm does not check if
an LCA table is in memory -- instead it builds LCA table for {\em
  every possible} block.  To do so, Demaine's algorithm uses a binary
string of length $2s$ to identify a block and its Cartesian tree.  The
binary string is encoded in such a way that one can answer an in-block
range maximum query by examining this binary string only.  However,
this examination requires counting the number of 1's {\em between} the
last two 0's, which is {\em hard} to implement efficiently in a modern
computer.

% We replace lookup operation to naive operation.  The naive operation
% is find the maximum value by comparing each element on the compressed
% data. On the other hand, the lookup operation find the index of
% maximum value from the ancestor table. When loading a element from index
% table, it also bring some useless data to caches.   In order to use
% cache efficiently, the naive operation is better than the lookup
% operation because the access pattern is almost one by one in our VGLCS
% algorithm.

\subsection{Rightmost-pops Encoding} \label{sec:cct}

We propose a new encoding for blocks, called {\em rightmost-pops},
instead of the binary string by Demaine~\cite{Demaine2009OnCT}, in order
to improve the performance of range maximum query.  This rightmost-pops
encoding is inspired by Demaine's algorithm and Cartesian tree.

The rightmost-pops encoding encodes a Cartesian tree by keeping only
the {\em rightmost} path of the Cartesian tree in a {\em stack}, and
keeping track of the {\em number of pops} from the stack when we add
data into the Cartesian tree.  Please refer to
Figure~\ref{fig:interval-cartesian} for an illustration.  To maintain
the heap property of the Cartesian tree, when we add the $i$-th data
$a_i$ into the Cartesian tree, we need to {\em pop} the data in the
stack, which stores the rightmost path of the Cartesian tree, that are
{\em smaller} than $a_i$.  We keep popping data until the top of stack
is no less than $a_i$, then we push $a_i$ into the stack.  Let $t_i$
be the number of nodes that need to be popped, and it is easy to see
that $0 \le t_i < s$, where $s$ is the block size.  We use these
$t_i$, the number of pops on the right most path, to encode a
Cartesian tree.

Consider the example in Figure~\ref{fig:interval-cartesian}.  When we
insert $a_1 = 0$, we just insert it into the stack since $a_0 = 1$, no
data is popped, and $t_1$ is $0$.  When we insert $a_2 = 4$ we need to
pop both $a_0$ and $a_1$ out of the stack, since they are smaller than
$a_2$, so $t_2$ is $2$.  As defined earlier, the contents of the stack
is exactly the rightmost path of the Cartesian tree, and we keep these
$t_i$'s to represent a Cartesian tree.

\begin{figure}[!thb]
  \centering
  \includegraphics[width=\linewidth]{\GraphicPath/fig-cartesian-encoding-stack.pdf}
  \caption{Rightmost path in stack}
  \label{fig:interval-cartesian}
\end{figure}


The key idea of rightmost-pops encoding is that we can use $t_i$'s to
{\em implicitly identify} the Cartesian tree of this block of data, and
that we can answer in-block range queries simply by examining these
$t_i$'s.  Suppose we want to answer an in-block maximum query that
ranges from $l$ to $r$ with these $t_i$'s.  We maintain the number of
times data are {\em popped} from the stack in a variable $x$, and
initialize $x$ to 0.  We then loop through $t_l$ to $t_r$ and let index
$j$ run from $l$ to $r$.  Every iteration adds 1 to $x$ then subtracts
$t_j$ from $x$.  We need to remember the index $j$ when $x$ becomes
smaller or equal to 0.  Finally, we report the {\em last} index $j$ when
$x$ becomes smaller or equal to 0, as the answer.  The pseudo code of
the query answering algorithm is in
Algorithm~\ref{alg:cartesian64bits-query}.  All the operations of
Algorithm~\ref{alg:cartesian64bits-query} map directly to machine
instructions so that unlike Demaine's algorithm,
Algorithm~\ref{alg:cartesian64bits-query} is extremely intuitive to
implement.

\input{\AlgoPath/alg-cartesian64bits-query-2e}

The correctness proof of Algorithm~\ref{alg:cartesian64bits-query} is
in Theorem~\ref{thm:correctness}.  The intuition is that we only need
to know the root of the tree when the last element in range was
inserted.

\begin{theorem} \label{thm:correctness}
  Algorithm~\ref{alg:cartesian64bits-query} correctly answers an
  in-block range maximum query.
\end{theorem}
\begin{proof}
When $x$, the number of poppings, is {\em smaller than or equal} to 0,
it means the added data has become the {\em root} of the tree of the
queried range.  As a result, when we add a data and it became the root
of the tree for the {\em last} time, the added data is indeed the
maximum among this interval, because according to the heap property, the
root is the maximum among all nodes within this tree.
\end{proof}

We also note that since all $t_i$'s are small than 16, we can represent
each of them as a 4-bit integer.  We then concatenate sixteen of these
4-bit integers into a 64-bit integer to present a Cartesian tree for a
block of sixteen data.  The pseudo code on how to build a 64-bit integer
to represent a block of 16 data is in
Algorithm~\ref{alg:cartesian-to-64bits}, which runs in time $O(s)$.
where $s$ is the block size.  Note that all the operations, e.g., shift,
addition, subtraction, in Algorithm~\ref{alg:cartesian-to-64bits} map
directly to machine instructions and are straightforward to implement.

\input{\AlgoPath/alg-cartesian-to-64bits-2e}

We choose the block size as $s = \frac{\log n}{4} = 16$ for performance
reason.  Modern CPUs support 64-bit register and fast operation on them.
When we pack 16 $t_i$ in to a 64-bit integer, we can leverage fast
64-bit instructions and improve performance.  In addition, we do {\em
not} build least common ancestor tables {\em explicitly} since we
implicitly maintain the Cartesian tree information within these 64-bit
$t_i$.  This approach reduces memory usage and improves cache
performance, it can also efficiently answer one-dimension range maximum
query for up to $n = 2^{64}$ data.

Note that our rightmost-pops encoding does improve cache performance,
but will increase the time complexity in answering queries.
Algorithm~\ref{alg:cartesian64bits-query} accesses data in a very
regular manner and has a better data locality than Fischer's algorithm.
The preprocessing time is $O(n)$, as same as in Fischer's algorithm.
However, a single query now needs $O(s)$ time, where $s$ is the block
size $\frac{\log n}{4}$.  This is acceptable in practice since we set
the block size $s$ to $16$ for $n$ up to $2^{64}$, so $s$ is a small
constant.  The space complexity is $O(n)$ as in Fischer's algorithm. The
overall time complexity of the parallel version of our VGLCS algorithm
becomes $O(n^2 \log{n} / p + n \log n)$.  Note that the $\log n$ comes
from the block size $s = O(\log n)$
