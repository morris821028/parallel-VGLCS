\section{Parallel VGLCS Algorithm} %
\label{sec:parallelVGLCS}

The serial VGLCS algorithm~\ref{alg:serial-VGLCS} by
Peng~\cite{Peng2011TheLC} and other variants of LCS are difficult to
parallelize.  These algorithms use several states to determine a new
state during dynamic programming.  This construction requires {\em
  heavy data dependency}, and makes it difficult to parallelize the
computation in a naive row-by-row manner.  If we parallel Peng's
algorithm with wavefront method intuitively, it will require extra
space to record all row status, which is not required in the origin
algorithm.  On the other hand, if we use the
Maleki's~\cite{Maleki2016EfficientPU} technique, it also uses extra
space to maintain state translation, and spends more time to merge
split parts.  Therefore, it is crucial that our parallelization
conserves both memory and time.

\input{algorithms/alg-serial-VGLCS}

In order to find VGLCS efficiently, we need to address the {\em
  incremental suffix maximum query} (ISMQ) problem, which was
proposed by Peng~\cite{Peng2011TheLC}.  A data structure that supports
incremental suffix maximum queries should support the following three
operations.  First, a {\tt make} operation creates an empty array $A$.
Second, an {\tt append(V)} operation appends a value $V$ to array $A$.
Finally an ISMQ {\tt query(x)} finds the {\em maximum} value among the
$x$-th value to the end of an array $A$.

The sequential version of Peng's algoroithm has two stages.  In the
first stage, the algorithm uses a {\em disjoint set} to efficiently
answer ISMQ to compute the answer on every column, which form an array
$S$.  In the second stage, the algorithm again uses ISMQ on array $S$
obtained from the first stage to get the final answers.

Peng uses a {\em disjoint-set} data structure to answer incremental
suffix maximum queries, and finds a VGLCS with answers from ISMQ. The
disjoint-set data structure by Gabow~\cite{Gabow1983ALA} and
Tarjan~\cite{Tarjan1975EfficiencyOA} solves the {\em union-and-find
problem}.  The amortized time per union/find operation is
$O(\alpha(n))$.

However, a disjoint set implementation of Peng's algorithm is difficult
to parallelize for two reasons.  First, the query in the second stage
will change the data structure because the lookup operation will
compress the path to the root, so it is difficult to maintain a
consistent view of the data structure when multiple processing units are
compressing the path simultaneously.  Second, in the first stage when
multiple processing units are compressing different paths, the load
among them could very different, and incurs load imbalance.  Third, in
the first stage there will be a large number of threads that work on
different part of the disjoint-set forests, therefore it will be
difficult to synchronize them efficiently.

Since the disjoint set cannot support ISMQ efficiently in parallel, we
consider the following data structures to support ISMQ efficiently in
parallel.

\begin{itemize}
  \item Segment tree~\cite{berg2000computational} supports ranged
    maximum query and update in multi-dimensions.  The time complemtiy
    of both update and query is $O(\log n)$ in one-dimension.
  \item Sparse table~\cite{Berkman1993RecursiveSP} requires a $O(n
    \log n)$ preprocessing, and can support ranged maximum query in
    $O(1)$ time in one dimensional data.  A sparse table is a two
    dimentional array.  The element of a sparse table in the $j$-th
    row and $i$-th column is the maximum among the $i$-th elelemnts
    and its $2^j - 1$ predecessors in the input array.
\end{itemize}

We give an example of the sparse table.  The input is in array $A$. We
split array $A$ into five blocks so that each block has four elements.
The we build a sparse tabel $ST$ on $A$ as desceibed earlier.  Now a
ranged maximum query on $A$ can be answered by at most two different
sub-queries.  If the query range maximum value in $[2, 13]$, it will
merge four maximum results $Q_L$, and $Q_R$.
    
\begin{figure}[!thb]
  \centering \subfigure[Array]{
    \includegraphics[width=\linewidth]{graphics/fig-interval-decomposition-origin.pdf}
    \label{fig:fig-interval-decomposition}
  } \subfigure[Sparse Table]{
    \includegraphics[width=\linewidth]{graphics/fig-sparse-table-origin.pdf}
    \label{fig:fig-sparse-table}
  }
  \caption{A sparse tabel example}
  \label{fig:interval-decomposition}
\end{figure}

We present our parallel algorithm for the VGLCS problem.  The algorithm
use {\em sparse table} and its time complexity is $O(n^2 / p + n \log
n)$, where $p$ is the number of processors.  In
Chapter~\ref{sec:parallelIRMQ}, we present a new data structure --
variant sparse table instead of disjoint set.  Also, we use them to
develop more stable algorithm in amortized $O(n^2)$ theoretically.

%% In order to support append value to tail, it cannot run $O(n)$ --
%% $O(1)$ solution which Fischer~\cite{Fischer2006TheoreticalAP}
%% introduced.

The pseudo code of our algorithm is in 
Algorithm~\ref{alg:parallel-VGLCS}

\iffalse 稀疏表是我們認為最好的替代方案，其整合後為 VGLCS 平行算法
\ref{alg:parallel-VGLCS}，算法的時間複雜度為 $O(n^2 / p + n \log n)$，
其中 $p$ 為處理器個數。在後續的章節，我們將提出新的數據結構取代并查集操
作，且能在平行算法達到理想複雜度 $O(n^2 / p + n \log n)$。\fi

\input{algorithms/alg-parallel-VGLCS}
