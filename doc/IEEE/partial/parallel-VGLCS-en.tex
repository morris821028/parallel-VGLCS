\section{Parallel VGLCS Algorithm} %
\label{sec:parallelVGLCS}

\input{algorithms/alg-serial-VGLCS}

The serial VGLCS algorithm~\ref{alg:serial-VGLCS} by
Peng~\cite{Peng2011TheLC} and other variants of LCS are difficult to
parallelize.  These algorithms use several states to determine a new
state during dynamic programming.  This construction requires {\em
  heavy data dependency}, and makes it difficult to parallelize the
computation in a naive row-by-row manner.  If we parallel Peng's
algorithm with wavefront method intuitively, it will require extra
space to record all row status, which is not required in the origin
algorithm.  On the other hand, if we use the
Maleki's~\cite{Maleki2016EfficientPU} technique, it also uses extra
space to maintain state translation, and spends more time to merge
split parts.  Therefore, it is crucial that our parallelization
conserves both memory and time.

In order to find VGLCS efficiently, we need to address the {\em
  incremental suffix maximum query} (ISMQ) problem, which was
proposed by Peng~\cite{Peng2011TheLC}.  A data structure that supports
incremental suffix maximum queries should support the following three
operations.  First, a {\tt make} operation creates an empty array $A$.
Second, an {\tt append(V)} operation appends a value $V$ to array $A$.
Finally an ISMQ {\tt query(x)} finds the {\em maximum} value among the
$x$-th value to the end of an array $A$.

The sequential version of Peng's algorithm has two stages.  In the
first stage, the algorithm uses a {\em disjoint set} to efficiently
answer ISMQ to compute the answer on every column, which form an array
$S$.  In the second stage, the algorithm again uses ISMQ on array $S$
obtained from the first stage to get the final answers.

Peng uses a {\em disjoint-set} data structure to answer incremental
suffix maximum queries, and finds a VGLCS with answers from ISMQ. The
disjoint-set data structure by Gabow~\cite{Gabow1983ALA} and
Tarjan~\cite{Tarjan1975EfficiencyOA} solves the {\em union-and-find
  problem}.  The set of data are stored in a sequence of disjoint
sets, and maintain the property that the {\em maximum} of disjoint
sets are at the root and in {\em decreasing} order.  When we add a
value $x$ into the data structure, we put it at the end as a set of
itself.  Then we start joining (with union operation) from the last
set to its previous set until the maximum of the previous set is {\em
  larger}.  It is easy to see that the {\tt query(x)} operation is
simply a {\em find} operation that finds the root, which has the {\em
  maximum}, of the tree that $x$ belongs to.  The amortized time per
union/find operation is $O(\alpha(n))$.

However, a disjoint set implementation of Peng's algorithm is difficult
to parallelize for two reasons.  First, the query in the second stage
will change the data structure because the lookup operation will
compress the path to the root, so it is difficult to maintain a
consistent view of the data structure when multiple processing units are
compressing the path simultaneously.  Second, in the first stage when
multiple processing units are compressing different paths, the load
among them could very different, and incurs load imbalance.  Third, in
the first stage there will be a large number of threads that work on
different part of the disjoint-set forests, therefore it will be
difficult to synchronize them efficiently.

Since the disjoint set cannot support ISMQ efficiently in parallel, we
consider the following data structures to support ISMQ efficiently in
parallel.

\begin{itemize}
  \item Segment tree~\cite{berg2000computational} supports ranged
    maximum query and update in multi-dimensions.  The time complexity
    of both update and query is $O(\log n)$ in one-dimension.
  \item Sparse table~\cite{Berkman1993RecursiveSP} requires a $O(n
    \log n)$ preprocessing, and can support ranged maximum query in
    $O(1)$ time on one dimensional data.  A sparse table is a two
    dimensional array.  The element of a sparse table in the $j$-th
    row and $i$-th column is the maximum among the $i$-th elements
    and its $2^j - 1$ predecessors in the input array.
\end{itemize}

\begin{figure}[!thb]
  \centering \subfigure[Array]{
    \includegraphics[width=\linewidth]{graphics/fig-interval-decomposition-origin.pdf}
    \label{fig:fig-interval-decomposition}
  } \subfigure[Sparse Table]{
    \includegraphics[width=\linewidth]{graphics/fig-sparse-table-origin.pdf}
    \label{fig:fig-sparse-table}
  }
  \caption{A sparse table example}
  \label{fig:interval-decomposition}
\end{figure}

We give an example of the sparse table.  The input is in array $A$. We
split array $A$ into five blocks so that each block has four elements.
The we build a sparse table $ST$ on $A$ as described earlier.  Now a
ranged maximum query on $A$ can be answered by at most {\em two}
queries into the sparse table.  For example, if the query is of the
range from 2 to 13, then the answer is the maximum of the two answers
-- one from 2 to 9, and one from 6 to 13.

We now present a simple version of our parallel VGLCS algorithm.  The
algorithm uses a sparse table and its time complexity is $O(n^2 \log n
/ p + n \log n)$, where $p$ is the number of processors.  In
Section~\ref{sec:parallelIRMQ}, we present a more complicated version
that uses a variant of the sparse table, and runs in $O(n^2 / p + n
\log n)$.

The operations on a sparse table are much easily to parallelize than
those on a disjoint, which is used in the second stage of Peng's
sequential VGLCS algorithm.  The second stage of Peng's algorithm
alternates between append and query operations.  This alternation
between append and query incurs heavy data dependency.  In addition,
the parallelism of operations on a disjoint tree is limited by the
length of path under compression.  The length is usually very short
and provide very limited parallelism.  In contrast our parallel sparse
table implementation in Algorithm~\ref{alg:parallel-sparse-table} runs
in only $O(n \log n / p + \log n)$, where $n$ is the number of
elements and $p$ is the number of processors, and is very easy to
parallelize and to implement.

\input{algorithms/alg-parallel-sparse-table}

The pseudo code of our simple version parallel VGLCS algorithm is in
Algorithm~\ref{alg:parallel-VGLCS}

\input{algorithms/alg-parallel-VGLCS}
